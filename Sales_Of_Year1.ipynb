{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4dfd19-a6d5-4108-8366-d6023465ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f6cf27-b282-4af7-b790-3803b6a8ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8166d7ed-f6d2-41b9-a542-a0db665533ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  141234|              iPhone|               1|     700.0|01/22/19 21:25|944 Walnut St, Bo...|\n",
      "|  141235|Lightning Chargin...|               1|     14.95|01/28/19 14:15|185 Maple St, Por...|\n",
      "|  141236|    Wired Headphones|               2|     11.99|01/17/19 13:33|538 Adams St, San...|\n",
      "|  141237|    27in FHD Monitor|               1|    149.99|01/05/19 20:33|738 10th St, Los ...|\n",
      "|  141238|    Wired Headphones|               1|     11.99|01/25/19 11:59|387 10th St, Aust...|\n",
      "|  141239|AAA Batteries (4-...|               1|      2.99|01/29/19 20:22|775 Willow St, Sa...|\n",
      "|  141240|27in 4K Gaming Mo...|               1|    389.99|01/26/19 12:16|979 Park St, Los ...|\n",
      "|  141241|USB-C Charging Cable|               1|     11.95|01/05/19 12:04|181 6th St, San F...|\n",
      "|  141242|Bose SoundSport H...|               1|     99.99|01/01/19 10:30|867 Willow St, Lo...|\n",
      "|  141243|Apple Airpods Hea...|               1|     150.0|01/22/19 21:20|657 Johnson St, S...|\n",
      "|  141244|Apple Airpods Hea...|               1|     150.0|01/07/19 11:29|492 Walnut St, Sa...|\n",
      "|  141245|  Macbook Pro Laptop|               1|    1700.0|01/31/19 10:12|322 6th St, San F...|\n",
      "|  141246|AAA Batteries (4-...|               3|      2.99|01/09/19 18:57|618 7th St, Los A...|\n",
      "|  141247|    27in FHD Monitor|               1|    149.99|01/25/19 19:19|512 Wilson St, Sa...|\n",
      "|  141248|       Flatscreen TV|               1|     300.0|01/03/19 21:54|363 Spruce St, Au...|\n",
      "|  141249|    27in FHD Monitor|               1|    149.99|01/05/19 17:20|440 Cedar St, Por...|\n",
      "|  141250|     Vareebadd Phone|               1|     400.0|01/10/19 11:20|471 Center St, Lo...|\n",
      "|  141251|Apple Airpods Hea...|               1|     150.0|01/24/19 08:13|414 Walnut St, Bo...|\n",
      "|  141252|USB-C Charging Cable|               1|     11.95|01/30/19 09:28|220 9th St, Los A...|\n",
      "|  141253|AA Batteries (4-p...|               1|      3.84|01/17/19 00:09|385 11th St, Atla...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_January_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_January_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_January_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19044ad-6c10-4973-bdb6-5b28dfec0101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 9723\n"
     ]
    }
   ],
   "source": [
    "Sales_January_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_January_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3146bac1-f8e7-4820-80ef-63396791406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|              9681|        9697|               9681|              9681|          9697|                9697|\n",
      "|   mean|145859.98481561823|        null|  1.122611300485487| 186.4490920359388|          null|                null|\n",
      "| stddev|2674.3333623799563|        null|0.44227300597063557|330.79969319883986|          null|                null|\n",
      "|    min|            141234|20in Monitor|                  1|              2.99|01/01/19 03:07|1 4th St, Los Ang...|\n",
      "|    max|            150501|      iPhone|                  7|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_January_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b5969f-b279-43b1-ab0f-f22474f4692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 42\n",
      "Product: 26\n",
      "Quantity Ordered: 42\n",
      "Price Each: 42\n",
      "Order Date: 26\n",
      "Purchase Address: 26\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_January_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_January_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_January_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e4d5be6-2d31-4806-bd26-a8765b47bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': USB-C Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 11.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_January_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3108b4be-1b6b-4d34-88a2-395152beec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_January_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_January_df = Sales_January_df.withColumn(col_name, when(Sales_January_df[col_name].isNull(), mean_val).otherwise(Sales_January_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80002516-bc78-4aa6-b9b9-be81d565c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.27%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Order Date': 0.27%\n",
      "Null percentage in column 'Purchase Address': 0.27%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_January_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_January_df.columns:\n",
    "    null_count = Sales_January_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49d5544-83b8-4a55-9e84-293f6e8b8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_January_df = Sales_January_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cd41add-cb59-4db6-8568-4d857f5c6986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.27%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.27%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_January_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_January_df.columns:\n",
    "    null_count = Sales_January_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7d655c7-ecce-4314-805f-33db4c2fcc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: USB-C Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_January_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64a7a709-9592-4413-9025-cd94c2235890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values with the most frequent value\n",
    "Sales_January_df = Sales_January_df.fillna(most_frequent_value, subset=[\"Product\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a36fad23-986f-4560-907a-3391593a072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.27%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_January_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_January_df.columns:\n",
    "    null_count = Sales_January_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cfee7e8-7f79-46bd-b0c8-87ed1f56f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_January_df.groupBy(\"Purchase Address\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Purchase Address\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51637733-5404-48a8-97a2-fb0c09496494",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"706 Johnson St, New York City\"\n",
    "Sales_January_df = Sales_January_df.fillna(default_value, subset=['Purchase Address'])\n",
    "# Sales_January_df= Sales_January_df.fillna(default_value2, subset=['image', 'previewLink','infoLink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5528df42-40e4-4c60-b9e5-f8fdfb9dbc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_January_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_January_df.columns:\n",
    "    null_count = Sales_January_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d458245-c0e3-4b05-b99f-4fa4205b2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: double (nullable = true)\n",
      " |-- Product: string (nullable = false)\n",
      " |-- Quantity Ordered: double (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Purchase Address: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_January_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c3156d3-ff1d-49ed-84cd-38883621b49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 9723\n",
      "After drop dublication: 9723\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_January_df.count())\n",
    "Sales_January_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_January_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e44b98-3b81-40dc-a7c1-93822d1279a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79d6f34-d615-4cbb-adb7-1029695c8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f752a792-4bd5-4575-91fe-f732691430a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a176d57e-2fac-416d-a4dd-744e8405caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  150502|              iPhone|               1|     700.0|02/18/19 01:35|866 Spruce St, Po...|\n",
      "|  150503|AA Batteries (4-p...|               1|      3.84|02/13/19 07:24|18 13th St, San F...|\n",
      "|  150504|27in 4K Gaming Mo...|               1|    389.99|02/18/19 09:46|52 6th St, New Yo...|\n",
      "|  150505|Lightning Chargin...|               1|     14.95|02/02/19 16:47|129 Cherry St, At...|\n",
      "|  150506|AA Batteries (4-p...|               2|      3.84|02/28/19 20:32|548 Lincoln St, S...|\n",
      "|  150507|Lightning Chargin...|               1|     14.95|02/24/19 18:50|387 12th St, Aust...|\n",
      "|  150508|AA Batteries (4-p...|               1|      3.84|02/21/19 19:26|622 Center St, Sa...|\n",
      "|  150509|Apple Airpods Hea...|               1|     150.0|02/26/19 19:53|921 6th St, Seatt...|\n",
      "|  150510|USB-C Charging Cable|               1|     11.95|02/17/19 21:48|451 2nd St, Los A...|\n",
      "|  150511|USB-C Charging Cable|               1|     11.95|02/22/19 07:36|689 River St, San...|\n",
      "|  150512|Bose SoundSport H...|               1|     99.99|02/17/19 18:29|198 Center St, Lo...|\n",
      "|  150513|Bose SoundSport H...|               1|     99.99|02/25/19 20:49|777 Spruce St, Lo...|\n",
      "|  150514|    27in FHD Monitor|               1|    149.99|02/03/19 00:21|723 Wilson St, Lo...|\n",
      "|  150515|Apple Airpods Hea...|               1|     150.0|02/18/19 14:53|101 13th St, New ...|\n",
      "|  150516|Lightning Chargin...|               1|     14.95|02/20/19 12:29|303 Sunset St, At...|\n",
      "|  150517|    Wired Headphones|               1|     11.99|02/08/19 12:57|471 13th St, San ...|\n",
      "|  150518|  Macbook Pro Laptop|               1|    1700.0|02/26/19 12:38|847 10th St, San ...|\n",
      "|  150518|              iPhone|               1|     700.0|02/26/19 12:38|847 10th St, San ...|\n",
      "|  150519|    Wired Headphones|               1|     11.99|02/23/19 13:25|111 Hill St, Dall...|\n",
      "|  150520|AA Batteries (4-p...|               2|      3.84|02/27/19 14:39|512 Church St, Da...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_February_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_February_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_February_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74c6bd8-3a15-44a8-a67a-267172d997b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 12036\n"
     ]
    }
   ],
   "source": [
    "Sales_February_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_February_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1097d72a-cf53-4751-bdc8-1e8b37767c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|  Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|  count|             11986|       12004|             11986|             11986|         12004|               12004|\n",
      "|   mean|156250.61338227932|        null|1.1230602369431002|182.74150675788204|          null|                null|\n",
      "| stddev|3322.0752634362825|        null|0.4311103873526451|325.54329574605885|          null|                null|\n",
      "|    min|            150502|20in Monitor|                 1|              2.99|02/01/19 01:51|1 Hill St, Boston...|\n",
      "|    max|            162008|      iPhone|                 7|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_February_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6819250e-52a3-446c-82dc-7d6dc6739df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 50\n",
      "Product: 32\n",
      "Quantity Ordered: 50\n",
      "Price Each: 50\n",
      "Order Date: 32\n",
      "Purchase Address: 32\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_February_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_February_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_February_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff996bb9-8b71-4281-911e-ad09cbf49240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': USB-C Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 11.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_February_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ea5698-67fa-47fa-8c65-b2358878880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_February_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_February_df = Sales_February_df.withColumn(col_name, when(Sales_February_df[col_name].isNull(), mean_val).otherwise(Sales_February_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c981c92-5996-47ed-aaf4-4a757d7dd894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.27%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Order Date': 0.27%\n",
      "Null percentage in column 'Purchase Address': 0.27%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_February_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_February_df.columns:\n",
    "    null_count = Sales_February_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927cd2df-8e62-4165-8fa8-46a805ca472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_February_df = Sales_February_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ba99f1c-dabe-4a00-b693-98626849928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: USB-C Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_February_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06d5d1f8-4a48-49cd-9455-f0bbb7e5256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_February_df = Sales_February_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67495c8-205e-4262-8ed5-fb24640d7756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.27%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_February_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_February_df.columns:\n",
    "    null_count = Sales_February_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c15354b-48e3-4ba4-92cc-da7b45b02567",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"777 Spruce St, Los Angeles\"\n",
    "Sales_February_df = Sales_February_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c146d33c-266f-4de4-ad2e-f12df43b95fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_February_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_February_df.columns:\n",
    "    null_count = Sales_February_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1834aac-4385-418e-a2ac-0387264041c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 12036\n",
      "After drop dublication: 12036\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_February_df.count())\n",
    "Sales_February_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_February_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ef4d2-afeb-4b16-998c-5cb9fb9777e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6c48fc-8308-424d-a8ef-a8291b6f6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c67860-4d20-495b-9e5a-90e4002f233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e55b2c5-bce5-43fe-86b0-d8a1b4113872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  162009|              iPhone|               1|     700.0|03/28/19 20:59|942 Church St, Au...|\n",
      "|  162009|Lightning Chargin...|               1|     14.95|03/28/19 20:59|942 Church St, Au...|\n",
      "|  162009|    Wired Headphones|               2|     11.99|03/28/19 20:59|942 Church St, Au...|\n",
      "|  162010|Bose SoundSport H...|               1|     99.99|03/17/19 05:39|261 10th St, San ...|\n",
      "|  162011|34in Ultrawide Mo...|               1|    379.99|03/10/19 00:01|764 13th St, San ...|\n",
      "|  162012|AA Batteries (4-p...|               1|      3.84|03/20/19 21:33|187 Ridge St, San...|\n",
      "|  162013|34in Ultrawide Mo...|               1|    379.99|03/15/19 23:05|904 Main St, Aust...|\n",
      "|  162014|USB-C Charging Cable|               1|     11.95|03/01/19 21:33|10 13th St, San F...|\n",
      "|  162015|AA Batteries (4-p...|               1|      3.84|03/02/19 08:52|949 Jefferson St,...|\n",
      "|  162016|AAA Batteries (4-...|               5|      2.99|03/19/19 21:10|469 Highland St, ...|\n",
      "|  162017|Lightning Chargin...|               1|     14.95|03/02/19 13:55|449 River St, San...|\n",
      "|  162018|USB-C Charging Cable|               2|     11.95|03/18/19 11:07|925 Maple St, Por...|\n",
      "|  162019|AAA Batteries (4-...|               1|      2.99|03/05/19 12:20|639 4th St, Atlan...|\n",
      "|  162020|  LG Washing Machine|               1|     600.0|03/26/19 18:17|616 Jackson St, D...|\n",
      "|  162021|    Wired Headphones|               1|     11.99|03/30/19 12:02|645 Wilson St, Lo...|\n",
      "|  162022|AA Batteries (4-p...|               1|      3.84|03/17/19 22:18|473 Hill St, New ...|\n",
      "|  162023|Apple Airpods Hea...|               1|     150.0|03/11/19 20:00|404 Park St, San ...|\n",
      "|  162024|27in 4K Gaming Mo...|               1|    389.99|03/17/19 09:22|952 Willow St, At...|\n",
      "|  162025|AAA Batteries (4-...|               1|      2.99|03/07/19 20:50|110 7th St, Bosto...|\n",
      "|  162026|              iPhone|               1|     700.0|03/22/19 22:31|71 Pine St, Austi...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_March_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_March_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_March_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc2aae1e-536a-41eb-aeb5-0781d0830715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 15226\n"
     ]
    }
   ],
   "source": [
    "Sales_March_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_March_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67097037-b310-47d6-a3d7-245e3520608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|             15154|       15189|              15154|             15154|         15189|               15189|\n",
      "|   mean|169275.36168668338|        null| 1.1222119572390128|184.31909726807467|          null|                null|\n",
      "| stddev| 4202.535663179397|        null|0.44120144094170255| 331.6197142860147|          null|                null|\n",
      "|    min|            162009|20in Monitor|                  1|              2.99|03/01/19 03:15|1 11th St, Atlant...|\n",
      "|    max|            176557|      iPhone|                  7|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_March_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e860026-b3d7-44e5-8720-4467a3408870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 72\n",
      "Product: 37\n",
      "Quantity Ordered: 72\n",
      "Price Each: 72\n",
      "Order Date: 37\n",
      "Purchase Address: 37\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_March_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_March_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_March_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205e7b09-f584-439f-9ae4-b0f9c36f8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': USB-C Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 11.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_March_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f85c90-18bd-4be5-9f52-c973554e95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_March_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_March_df = Sales_March_df.withColumn(col_name, when(Sales_March_df[col_name].isNull(), mean_val).otherwise(Sales_March_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c75883d-5c3c-4623-bb22-b985a536cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_March_df = Sales_March_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1785ebd-6178-4ad7-93d0-c464fad719ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: USB-C Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_March_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6153467-da9d-46ab-ae4f-c2c0db1401e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_March_df = Sales_March_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe7767b-5014-4382-9c5d-1cfefd3d0b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.24%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_March_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_March_df.columns:\n",
    "    null_count = Sales_March_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30b7af42-3152-4105-a1f7-822551e637ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"777 Spruce St, Los Angeles\"\n",
    "Sales_March_df = Sales_March_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1322ee91-c235-4667-b605-1211fbf0e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_March_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_March_df.columns:\n",
    "    null_count = Sales_March_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1227ff80-eb74-4b13-889e-8881856a92ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 15226\n",
      "After drop dublication: 15226\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_March_df.count())\n",
    "Sales_March_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_March_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba08c7-51be-45eb-8bd0-ef3a773948c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43339098-78f9-4ec2-9d41-db6256e7c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51040371-0e53-4c2d-a489-bfebbadf623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b77690d-8efe-47bd-acf5-6f653c5ebfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  176558|USB-C Charging Cable|               2|     11.95|04/19/19 08:46|917 1st St, Dalla...|\n",
      "|    null|                null|            null|      null|          null|                null|\n",
      "|  176559|Bose SoundSport H...|               1|     99.99|04/07/19 22:30|682 Chestnut St, ...|\n",
      "|  176560|        Google Phone|               1|     600.0|04/12/19 14:38|669 Spruce St, Lo...|\n",
      "|  176560|    Wired Headphones|               1|     11.99|04/12/19 14:38|669 Spruce St, Lo...|\n",
      "|  176561|    Wired Headphones|               1|     11.99|04/30/19 09:27|333 8th St, Los A...|\n",
      "|  176562|USB-C Charging Cable|               1|     11.95|04/29/19 13:03|381 Wilson St, Sa...|\n",
      "|  176563|Bose SoundSport H...|               1|     99.99|04/02/19 07:46|668 Center St, Se...|\n",
      "|  176564|USB-C Charging Cable|               1|     11.95|04/12/19 10:58|790 Ridge St, Atl...|\n",
      "|  176565|  Macbook Pro Laptop|               1|    1700.0|04/24/19 10:38|915 Willow St, Sa...|\n",
      "|  176566|    Wired Headphones|               1|     11.99|04/08/19 14:05|83 7th St, Boston...|\n",
      "|  176567|        Google Phone|               1|     600.0|04/18/19 17:18|444 7th St, Los A...|\n",
      "|  176568|Lightning Chargin...|               1|     14.95|04/15/19 12:18|438 Elm St, Seatt...|\n",
      "|  176569|27in 4K Gaming Mo...|               1|    389.99|04/16/19 19:23|657 Hill St, Dall...|\n",
      "|  176570|AA Batteries (4-p...|               1|      3.84|04/22/19 15:09|186 12th St, Dall...|\n",
      "|  176571|Lightning Chargin...|               1|     14.95|04/19/19 14:29|253 Johnson St, A...|\n",
      "|  176572|Apple Airpods Hea...|               1|     150.0|04/04/19 20:30|149 Dogwood St, N...|\n",
      "|  176573|USB-C Charging Cable|               1|     11.95|04/27/19 18:41|214 Chestnut St, ...|\n",
      "|  176574|        Google Phone|               1|     600.0|04/03/19 19:42|20 Hill St, Los A...|\n",
      "|  176574|USB-C Charging Cable|               1|     11.95|04/03/19 19:42|20 Hill St, Los A...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_April_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_April_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_April_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903053f9-4df1-498d-a28b-5334709b52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_April_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_April_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bff14b0-cab7-4138-bbea-aaf414a8c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|             18289|       18324|              18289|             18289|         18324|               18324|\n",
      "|   mean|185328.81672043304|        null| 1.1246104215648751|184.43102630000277|          null|                null|\n",
      "| stddev| 5061.520829296985|        null|0.43640973695741925| 330.9133771769665|          null|                null|\n",
      "|    min|            176558|20in Monitor|                  1|              2.99|04/01/19 03:09|1 14th St, New Yo...|\n",
      "|    max|            194094|      iPhone|                  7|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_April_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2de5d01-6a8c-4174-8973-9d6c7724e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 94\n",
      "Product: 59\n",
      "Quantity Ordered: 94\n",
      "Price Each: 94\n",
      "Order Date: 59\n",
      "Purchase Address: 59\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_April_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_April_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_April_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88289fe6-82d4-4070-8536-0b3f51c94c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': Lightning Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 14.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_April_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed8a2853-f4e5-4fe2-b6fd-a2dd3db61302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_April_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_April_df = Sales_April_df.withColumn(col_name, when(Sales_April_df[col_name].isNull(), mean_val).otherwise(Sales_April_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddc374b-a7ce-477a-ad65-cac16dfa2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_April_df = Sales_April_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39af9f5-8c14-4091-8513-17da167d84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: Lightning Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_April_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bccbde12-c58f-4b00-8553-52c321b8cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_April_df = Sales_April_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c2ed847-fc0f-48b5-a01c-a917be59ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.32%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_April_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_April_df.columns:\n",
    "    null_count = Sales_April_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3598f22-3788-4fbc-8e46-d8c66140f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"874 Jefferson St, San Francisco, CA 94016\"\n",
    "Sales_April_df = Sales_April_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f5c9ff3-c48c-4b62-9f2e-74eb0afdab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_April_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_April_df.columns:\n",
    "    null_count = Sales_April_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d72a3d-4bb0-48dc-b777-9d00f6910117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 18383\n",
      "After drop dublication: 18383\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_April_df.count())\n",
    "Sales_April_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_April_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4164a8-886c-4f69-a35e-a0a6d6cc118d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3601b909-fd64-487d-a5d4-0728d40a7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef27e193-1c8d-4807-b758-4379448c64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e252888a-6876-4fcb-9efc-a0c1f622aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  194095|    Wired Headphones|               1|     11.99|05/16/19 17:14|669 2nd St, New Y...|\n",
      "|  194096|AA Batteries (4-p...|               1|      3.84|05/19/19 14:43|844 Walnut St, Da...|\n",
      "|  194097|    27in FHD Monitor|               1|    149.99|05/24/19 11:36|164 Madison St, N...|\n",
      "|  194098|    Wired Headphones|               1|     11.99|05/02/19 20:40|622 Meadow St, Da...|\n",
      "|  194099|AAA Batteries (4-...|               2|      2.99|05/11/19 22:55|17 Church St, Sea...|\n",
      "|  194100|              iPhone|               1|     700.0|05/10/19 19:44|81 Jefferson St, ...|\n",
      "|  194101|USB-C Charging Cable|               1|     11.95|05/11/19 22:44|354 Meadow St, Bo...|\n",
      "|  194102|Lightning Chargin...|               1|     14.95|05/07/19 12:49|166 4th St, Dalla...|\n",
      "|  194103|    27in FHD Monitor|               1|    149.99|05/24/19 18:28|759 Pine St, Seat...|\n",
      "|  194104|    Wired Headphones|               1|     11.99|05/04/19 18:38|330 1st St, Seatt...|\n",
      "|  194105|     ThinkPad Laptop|               1|    999.99|05/13/19 09:07|928 Hill St, Atla...|\n",
      "|  194106|USB-C Charging Cable|               1|     11.95|05/12/19 10:07|261 Jackson St, S...|\n",
      "|  194107|34in Ultrawide Mo...|               1|    379.99|05/19/19 17:52|451 Walnut St, Ne...|\n",
      "|  194108|AA Batteries (4-p...|               1|      3.84|05/22/19 18:15|832 Lakeview St, ...|\n",
      "|  194109|AAA Batteries (4-...|               1|      2.99|05/15/19 14:00|202 11th St, Los ...|\n",
      "|  194110|        Google Phone|               1|     600.0|05/31/19 18:07|498 14th St, Los ...|\n",
      "|  194110|    Wired Headphones|               1|     11.99|05/31/19 18:07|498 14th St, Los ...|\n",
      "|  194111|    27in FHD Monitor|               1|    149.99|05/04/19 15:08|99 Dogwood St, Da...|\n",
      "|  194112|        Google Phone|               1|     600.0|05/29/19 21:13|853 West St, Aust...|\n",
      "|  194113|Apple Airpods Hea...|               1|     150.0|05/10/19 22:33|29 Adams St, Los ...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_May_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_May_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_May_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f55d69-3cdb-4e5c-8582-a8d0239dc54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 16635\n"
     ]
    }
   ],
   "source": [
    "Sales_May_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_May_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317e1c1a-3f4f-40ed-81f7-a470c5adb769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|         Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|            16554|       16587|              16554|             16554|         16587|               16587|\n",
      "|   mean|201999.9504651444|        null|  1.127038782167452|188.90281502963637|          null|                null|\n",
      "| stddev|4570.749762299136|        null|0.44851106171692223|342.10246405616186|          null|                null|\n",
      "|    min|           194095|20in Monitor|                  1|              2.99|05/01/19 02:50|1 7th St, Los Ang...|\n",
      "|    max|           209920|      iPhone|                  7|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_May_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33edb2d7-32b6-4a80-b27a-da96efc57312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 81\n",
      "Product: 48\n",
      "Quantity Ordered: 81\n",
      "Price Each: 81\n",
      "Order Date: 48\n",
      "Purchase Address: 48\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_May_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_May_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_May_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf50a14f-701e-4deb-af39-69121a1a9f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': Lightning Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 14.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_May_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49fa080c-1c71-49ca-951e-1bce432064fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_May_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_May_df = Sales_May_df.withColumn(col_name, when(Sales_May_df[col_name].isNull(), mean_val).otherwise(Sales_May_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b48fc3-3527-4ef0-98af-95cc0b57084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_May_df = Sales_May_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c5ae2b-200f-4f91-b847-7e450be3a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: Lightning Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_May_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4cf28a-8171-44d9-b8bd-d1efada6dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_May_df = Sales_May_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4ea7bc6-d1d4-40e5-b7de-5801ff9d9bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.29%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_May_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_May_df.columns:\n",
    "    null_count = Sales_May_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ed6add-d30d-47ff-8ac8-0d919b203ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"270 Dogwood St, San Francisco\"\n",
    "Sales_May_df = Sales_May_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4220354c-88df-499c-8eb9-27d88370f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_May_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_May_df.columns:\n",
    "    null_count = Sales_May_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "816f232e-afcc-40a4-96b1-39d44eb8402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 16635\n",
      "After drop dublication: 16635\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_May_df.count())\n",
    "Sales_May_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_May_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2697d-1dc1-4a21-ad5b-be149d2ebf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdbbd1f-5503-4dea-9941-558bbec64609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db78d62-f2a2-4016-87ee-737437010950",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ea2687-b82e-4d90-b1e0-feba24a7ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  209921|USB-C Charging Cable|               1|     11.95|06/23/19 19:34|950 Walnut St, Po...|\n",
      "|  209922|  Macbook Pro Laptop|               1|    1700.0|06/30/19 10:05|80 4th St, San Fr...|\n",
      "|  209923|     ThinkPad Laptop|               1|    999.99|06/24/19 20:18|402 Jackson St, L...|\n",
      "|  209924|    27in FHD Monitor|               1|    149.99|06/05/19 10:21|560 10th St, Seat...|\n",
      "|  209925|Bose SoundSport H...|               1|     99.99|06/25/19 18:58|545 2nd St, San F...|\n",
      "|  209926|Apple Airpods Hea...|               1|     150.0|06/28/19 20:04|386 Lake St, Seat...|\n",
      "|  209927|Lightning Chargin...|               1|     14.95|06/28/19 00:07|29 Lincoln St, Lo...|\n",
      "|  209928|Apple Airpods Hea...|               1|     150.0|06/16/19 21:30|350 9th St, New Y...|\n",
      "|  209929|    Wired Headphones|               1|     11.99|06/28/19 10:56|612 Meadow St, Po...|\n",
      "|  209930|    27in FHD Monitor|               1|    149.99|06/02/19 11:22|625 Ridge St, Los...|\n",
      "|  209931|Apple Airpods Hea...|               1|     150.0|06/24/19 13:55|761 14th St, New ...|\n",
      "|  209932|       Flatscreen TV|               1|     300.0|06/12/19 14:36|858 8th St, Bosto...|\n",
      "|  209933|AA Batteries (4-p...|               1|      3.84|06/07/19 15:39|932 Lakeview St, ...|\n",
      "|  209934|AAA Batteries (4-...|               1|      2.99|06/13/19 20:53|387 Dogwood St, B...|\n",
      "|  209935|AAA Batteries (4-...|               1|      2.99|06/09/19 11:13|530 Cedar St, Bos...|\n",
      "|  209936|34in Ultrawide Mo...|               1|    379.99|06/15/19 12:21|769 Dogwood St, D...|\n",
      "|  209937|AA Batteries (4-p...|               1|      3.84|06/29/19 18:01|675 Maple St, Dal...|\n",
      "|  209938|              iPhone|               1|     700.0|06/15/19 12:29|766 Meadow St, Se...|\n",
      "|  209938|    Wired Headphones|               1|     11.99|06/15/19 12:29|766 Meadow St, Se...|\n",
      "|  209939|        Google Phone|               1|     600.0|06/08/19 16:55|313 South St, San...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_June_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_June_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_June_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43df8993-8b52-4595-a2bf-b83150a5cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 13622\n"
     ]
    }
   ],
   "source": [
    "Sales_June_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_June_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2cdd17-9b83-4009-8b89-a32e7f7dc6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|  Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|  count|             13556|       13579|             13556|             13556|         13579|               13579|\n",
      "|   mean|216411.62769253468|        null|1.1253319563293007|189.03043523164672|          null|                null|\n",
      "| stddev| 3753.481272755102|        null|0.4294627910676135| 336.8637764725025|          null|                null|\n",
      "|    min|            209921|20in Monitor|                 1|              2.99|06/01/19 04:52|1 1st St, Dallas,...|\n",
      "|    max|            222909|      iPhone|                 9|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_June_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920f82cc-635b-4a7c-a54a-4bdb91f3076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 66\n",
      "Product: 43\n",
      "Quantity Ordered: 66\n",
      "Price Each: 66\n",
      "Order Date: 43\n",
      "Purchase Address: 43\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_June_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_June_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_June_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b55c93-bf00-4189-ad51-2b664ebe7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': Lightning Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 14.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_June_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed658b4e-2484-4f49-99ea-ae8258fe840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_June_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_June_df = Sales_June_df.withColumn(col_name, when(Sales_June_df[col_name].isNull(), mean_val).otherwise(Sales_June_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0b2850-0818-4ad3-b25c-8d8fc8e4204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_June_df = Sales_June_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51a66e0-e9dd-46f5-892e-39b3807b79a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: Lightning Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_June_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ef19d8-8f95-4ad4-aa7c-0d79b9bf9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_June_df = Sales_June_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3517fcd5-74c7-4e12-9233-d8d1805bf03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.32%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_June_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_June_df.columns:\n",
    "    null_count = Sales_June_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01ae6e04-925f-4dbd-a64e-e7b1a132ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"820 Washington St, New York City\"\n",
    "Sales_June_df = Sales_June_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b28388-92a1-4005-b7e0-2acc920736d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_June_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_June_df.columns:\n",
    "    null_count = Sales_June_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60a37f9a-887b-4dd3-bb1f-2e850868ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 13622\n",
      "After drop dublication: 13622\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_June_df.count())\n",
    "Sales_June_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_June_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09a3be-5f71-4ecc-a178-029741a15a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4168048-87c4-41fd-be86-1b707f802955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacc5fbb-c6d2-4dd1-ae6c-1eef9d968720",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80e1090-5c39-423e-8b68-d8a27434379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  222910|Apple Airpods Hea...|               1|     150.0|07/26/19 16:51|389 South St, Atl...|\n",
      "|  222911|       Flatscreen TV|               1|     300.0|07/05/19 08:55|590 4th St, Seatt...|\n",
      "|  222912|AA Batteries (4-p...|               1|      3.84|07/29/19 12:41|861 Hill St, Atla...|\n",
      "|  222913|AA Batteries (4-p...|               1|      3.84|07/28/19 10:15|190 Ridge St, Atl...|\n",
      "|  222914|AAA Batteries (4-...|               5|      2.99|07/31/19 02:13|824 Forest St, Se...|\n",
      "|  222915|Bose SoundSport H...|               1|     99.99|07/03/19 18:30|899 Elm St, San F...|\n",
      "|  222916|        Google Phone|               1|     600.0|07/21/19 22:39|745 Chestnut St, ...|\n",
      "|  222917|            LG Dryer|               1|     600.0|07/17/19 13:44|490 Adams St, New...|\n",
      "|  222918|USB-C Charging Cable|               1|     11.95|07/14/19 21:16|207 1st St, Los A...|\n",
      "|  222919|Lightning Chargin...|               1|     14.95|07/14/19 13:24|29 Jefferson St, ...|\n",
      "|  222920|AA Batteries (4-p...|               1|      3.84|07/30/19 04:42|590 6th St, San F...|\n",
      "|  222921|Lightning Chargin...|               1|     14.95|07/02/19 11:04|960 Willow St, Sa...|\n",
      "|  222922|34in Ultrawide Mo...|               1|    379.99|07/20/19 21:12|616 Chestnut St, ...|\n",
      "|  222923|AAA Batteries (4-...|               1|      2.99|07/05/19 12:16|105 Johnson St, L...|\n",
      "|  222924|Bose SoundSport H...|               1|     99.99|07/13/19 20:46|444 Spruce St, Sa...|\n",
      "|  222925|Apple Airpods Hea...|               1|     150.0|07/09/19 02:59|764 Adams St, San...|\n",
      "|  222926|Lightning Chargin...|               2|     14.95|07/15/19 17:44|577 Lakeview St, ...|\n",
      "|  222927|    Wired Headphones|               1|     11.99|07/31/19 20:34|244 Washington St...|\n",
      "|  222928|        Google Phone|               1|     600.0|07/15/19 17:18|524 Cherry St, Sa...|\n",
      "|  222929|     Vareebadd Phone|               1|     400.0|07/04/19 19:46|882 Hickory St, B...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_July_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_July_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_July_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0798772a-e8c0-4cb3-862e-734d879e238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 14371\n"
     ]
    }
   ],
   "source": [
    "Sales_July_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_July_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35ba655-1392-40ca-9fff-669212851382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|  Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|  count|             14291|       14326|             14291|             14291|         14326|               14326|\n",
      "|   mean| 229788.5162689805|        null|1.1244139668322721|184.14992302849797|          null|                null|\n",
      "| stddev|3970.6631212859775|        null|0.4608380693586461|332.95449992591784|          null|                null|\n",
      "|    min|            222910|20in Monitor|                 1|              2.99|07/01/19 06:08|1 4th St, Atlanta...|\n",
      "|    max|            236669|      iPhone|                 9|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_July_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2769baa3-e5a2-4769-9cdb-2ba438205b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 80\n",
      "Product: 45\n",
      "Quantity Ordered: 80\n",
      "Price Each: 80\n",
      "Order Date: 45\n",
      "Purchase Address: 45\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_July_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_July_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_July_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa2d327-6bd6-4a6f-9e1e-e41b41bb80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': Lightning Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 14.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_July_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b112318-32b3-4258-a7ba-1da18c16c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_July_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_July_df = Sales_July_df.withColumn(col_name, when(Sales_July_df[col_name].isNull(), mean_val).otherwise(Sales_July_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad12113-5cb8-491a-a4d8-88d11163961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_July_df = Sales_July_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f415666-99fa-447c-9147-1d0a1e5ff8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: Lightning Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_July_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3905ec8d-637e-43e4-a22f-8094e31a1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_July_df = Sales_July_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18eb9bdd-9e8e-49ca-bd0e-57da349e37bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.31%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_July_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_July_df.columns:\n",
    "    null_count = Sales_July_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "665df642-1fc9-44d8-ad0e-010934cd3992",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"511 12th St, Austin\"\n",
    "Sales_July_df = Sales_July_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a93c861-2d60-4e33-a816-3e601f915d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_July_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_July_df.columns:\n",
    "    null_count = Sales_July_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "556c2663-3e93-4b1b-abe6-b7030c53c988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 14371\n",
      "After drop dublication: 14371\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_July_df.count())\n",
    "Sales_July_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_July_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929bf97d-6a21-4fc2-9447-59284e52292e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad34541f-30de-4752-96d9-350b2cae5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a888dc9-1228-4995-8545-c05851ec81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fea2f8-1beb-445a-88b4-14200779ce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  236670|    Wired Headphones|               2|     11.99|08/31/19 22:21|359 Spruce St, Se...|\n",
      "|  236671|Bose SoundSport H...|               1|     99.99|08/15/19 15:11|492 Ridge St, Dal...|\n",
      "|  236672|              iPhone|               1|     700.0|08/06/19 14:40|149 7th St, Portl...|\n",
      "|  236673|AA Batteries (4-p...|               2|      3.84|08/29/19 20:59|631 2nd St, Los A...|\n",
      "|  236674|AA Batteries (4-p...|               2|      3.84|08/15/19 19:53|736 14th St, New ...|\n",
      "|  236675|    Wired Headphones|               1|     11.99|08/02/19 23:54|470 Hill St, San ...|\n",
      "|  236676|34in Ultrawide Mo...|               1|    379.99|08/04/19 19:52|470 Cherry St, Lo...|\n",
      "|  236677|        20in Monitor|               1|    109.99|08/13/19 07:16|918 6th St, San F...|\n",
      "|  236678|    Wired Headphones|               1|     11.99|08/25/19 20:11|58 9th St, San Fr...|\n",
      "|  236679|  Macbook Pro Laptop|               1|    1700.0|08/07/19 15:43|239 Spruce St, Lo...|\n",
      "|  236680|  LG Washing Machine|               1|     600.0|08/09/19 19:38|967 Willow St, Sa...|\n",
      "|  236681|AA Batteries (4-p...|               1|      3.84|08/26/19 20:52|295 1st St, Bosto...|\n",
      "|  236682|AA Batteries (4-p...|               1|      3.84|08/19/19 12:40|118 Johnson St, P...|\n",
      "|  236683|    27in FHD Monitor|               1|    149.99|08/31/19 15:47|196 West St, Dall...|\n",
      "|  236684|Lightning Chargin...|               1|     14.95|08/09/19 16:50|669 12th St, New ...|\n",
      "|  236685|Apple Airpods Hea...|               1|     150.0|08/23/19 19:29|238 Highland St, ...|\n",
      "|  236686|AAA Batteries (4-...|               1|      2.99|08/15/19 19:13|766 Maple St, Dal...|\n",
      "|  236687|USB-C Charging Cable|               1|     11.95|08/23/19 12:54|668 Meadow St, Ne...|\n",
      "|  236688|34in Ultrawide Mo...|               1|    379.99|08/08/19 16:06|821 7th St, Los A...|\n",
      "|  236689|AAA Batteries (4-...|               1|      2.99|08/21/19 10:55|13 Cedar St, San ...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_August_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_August_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_August_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45eb6cec-8a08-4edb-9eeb-58620913b9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 12011\n"
     ]
    }
   ],
   "source": [
    "Sales_August_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_August_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f3bf89-3129-43ba-950a-41115ce25d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|summary|         Order ID|     Product|  Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|  count|            11957|       11983|             11957|             11957|         11983|               11983|\n",
      "|   mean|242420.3392991553|        null| 1.124195032198712|186.52644308773156|          null|                null|\n",
      "| stddev|3313.683368107521|        null|0.4495797801138998| 332.3019347560669|          null|                null|\n",
      "|    min|           236670|20in Monitor|                 1|              2.99|08/01/19 04:50|1 2nd St, New Yor...|\n",
      "|    max|           248150|      iPhone|                 8|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_August_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02d1b4c-f34d-4f57-b869-2e8990c3cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 54\n",
      "Product: 28\n",
      "Quantity Ordered: 54\n",
      "Price Each: 54\n",
      "Order Date: 28\n",
      "Purchase Address: 28\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_August_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_August_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_August_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ddbe5a-a062-407e-a8a3-9a5ca5ddbadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': AA Batteries (4-pack)\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 3.84\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_August_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c746186-6849-4d3a-8a32-4fa3bc5d7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_August_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_August_df = Sales_August_df.withColumn(col_name, when(Sales_August_df[col_name].isNull(), mean_val).otherwise(Sales_August_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586b25f9-e856-4154-a591-1f78cc1ed3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_August_df = Sales_August_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1bd12a-40bb-4800-b76b-4e7f26915f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: AA Batteries (4-pack)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_August_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32e485e4-6dbc-47c0-a650-5221c7bec4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_August_df = Sales_August_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a907cf7-ddb2-46c6-9bcd-b1211ef6e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.23%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_August_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_August_df.columns:\n",
    "    null_count = Sales_August_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec0a26-d36a-4192-96da-9f121059b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"672 West St, Seattle\"\n",
    "Sales_February_df = Sales_February_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c44605f-870b-4c9c-99d1-ef501a51e288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.23%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_August_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_August_df.columns:\n",
    "    null_count = Sales_August_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1811adf7-07c3-497c-bed6-a534cef9adbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 12011\n",
      "After drop dublication: 12011\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_August_df.count())\n",
    "Sales_August_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_August_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa34f50-f89b-4917-9b3e-356872dd3357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0d5829-049e-4d1f-9fda-6791cd64ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc6f654-c974-4002-9416-c324ba409e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bccd159-0606-4869-baf5-ae328c216bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  248151|AA Batteries (4-p...|               4|      3.84|09/17/19 14:44|380 North St, Los...|\n",
      "|  248152|USB-C Charging Cable|               2|     11.95|09/29/19 10:19|511 8th St, Austi...|\n",
      "|  248153|USB-C Charging Cable|               1|     11.95|09/16/19 17:48|151 Johnson St, L...|\n",
      "|  248154|    27in FHD Monitor|               1|    149.99|09/27/19 07:52|355 Hickory St, S...|\n",
      "|  248155|USB-C Charging Cable|               1|     11.95|09/01/19 19:03|125 5th St, Atlan...|\n",
      "|  248156|34in Ultrawide Mo...|               1|    379.99|09/13/19 14:59|469 12th St, Los ...|\n",
      "|  248157|Lightning Chargin...|               1|     14.95|09/07/19 09:59|773 Johnson St, P...|\n",
      "|  248158|Lightning Chargin...|               1|     14.95|09/02/19 14:16|682 Sunset St, Lo...|\n",
      "|  248159|     Vareebadd Phone|               1|     400.0|09/06/19 16:45|664 Wilson St, Ne...|\n",
      "|  248160|    Wired Headphones|               1|     11.99|09/01/19 22:03|446 9th St, San F...|\n",
      "|  248161|USB-C Charging Cable|               1|     11.95|09/24/19 16:26|439 Spruce St, Sa...|\n",
      "|  248162|AAA Batteries (4-...|               3|      2.99|09/14/19 12:52|439 Walnut St, Se...|\n",
      "|  248163|USB-C Charging Cable|               1|     11.95|09/25/19 12:32|41 North St, Port...|\n",
      "|  248164|    27in FHD Monitor|               1|    149.99|09/12/19 15:57|88 9th St, San Fr...|\n",
      "|  248165|Lightning Chargin...|               1|     14.95|09/15/19 12:42|585 Adams St, Atl...|\n",
      "|  248166|Apple Airpods Hea...|               1|     150.0|09/04/19 14:59|21 9th St, Boston...|\n",
      "|  248167|        Google Phone|               1|     600.0|09/02/19 18:52|711 Forest St, Sa...|\n",
      "|  248168|USB-C Charging Cable|               1|     11.95|09/30/19 18:59|219 Hickory St, S...|\n",
      "|  248169|        20in Monitor|               1|    109.99|09/24/19 19:40|140 Ridge St, San...|\n",
      "|  248170|AAA Batteries (4-...|               1|      2.99|09/04/19 02:01|857 Main St, Bost...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_September_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_September_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_September_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4587070-81ba-4975-986e-bf760e1628c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 11686\n"
     ]
    }
   ],
   "source": [
    "Sales_September_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_September_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d16c1c9-1c93-4069-ad8d-e99eba88ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|             11629|       11646|              11629|             11629|         11646|               11646|\n",
      "|   mean|253751.81442944362|        null| 1.1281279559721387|179.40000687934585|          null|                null|\n",
      "| stddev| 3235.175358525277|        null|0.43507719933866423|328.59504155699716|          null|                null|\n",
      "|    min|            248151|20in Monitor|                  1|              2.99|09/01/19 05:10|1 11th St, San Fr...|\n",
      "|    max|            259357|      iPhone|                  6|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_September_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dedc957-7ef9-4569-a62d-3873ae909870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 57\n",
      "Product: 40\n",
      "Quantity Ordered: 57\n",
      "Price Each: 57\n",
      "Order Date: 40\n",
      "Purchase Address: 40\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_September_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_September_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_September_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a656e8-29ae-4f8d-a9ea-06fc4c43ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': USB-C Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 11.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_September_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a260db-6fca-4c9f-aaae-96a7933dfd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_September_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_September_df = Sales_September_df.withColumn(col_name, when(Sales_September_df[col_name].isNull(), mean_val).otherwise(Sales_September_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e8799b-6c51-4f22-be4f-1ab59e67a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_September_df = Sales_September_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d281ce0-e5ce-4972-9618-a4770f560e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: USB-C Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_September_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8baef9-1ffe-4ba6-89bd-4ac5c435f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_September_df = Sales_September_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99b1fc6e-bcdb-45ae-ac02-f04ac21a5f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.34%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_September_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_September_df.columns:\n",
    "    null_count = Sales_September_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c9565d-04d0-4ee3-bff4-2b6d7a882227",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"170 Park St, Boston\"\n",
    "Sales_September_df = Sales_September_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4acf49df-6c04-4ab7-87a7-25783bc25e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_September_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_September_df.columns:\n",
    "    null_count = Sales_September_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b10d0dd-0eda-4f5a-8890-d194798f135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 11686\n",
      "After drop dublication: 11686\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_September_df.count())\n",
    "Sales_September_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_September_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1735f9-bc01-44f9-b5e2-f71984a3823f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab95676-5245-499e-af09-e73c2f92b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5f68a9-7161-4eee-a8ae-411ac347e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96913fe9-888a-4656-bb46-35a9fb6e8965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  259358|34in Ultrawide Mo...|               1|    379.99|10/28/19 10:56|609 Cherry St, Da...|\n",
      "|  259359|27in 4K Gaming Mo...|               1|    389.99|10/28/19 17:26|225 5th St, Los A...|\n",
      "|  259360|AAA Batteries (4-...|               2|      2.99|10/24/19 17:20|967 12th St, New ...|\n",
      "|  259361|    27in FHD Monitor|               1|    149.99|10/14/19 22:26|628 Jefferson St,...|\n",
      "|  259362|    Wired Headphones|               1|     11.99|10/07/19 16:10|534 14th St, Los ...|\n",
      "|  259363|AAA Batteries (4-...|               1|      2.99|10/01/19 18:55|976 Lake St, New ...|\n",
      "|  259364|    Wired Headphones|               1|     11.99|10/29/19 11:02|874 North St, Los...|\n",
      "|  259365|Lightning Chargin...|               1|     14.95|10/29/19 11:19|127 12th St, Los ...|\n",
      "|  259366|Apple Airpods Hea...|               1|     150.0|10/20/19 11:52|955 9th St, Los A...|\n",
      "|  259367|Apple Airpods Hea...|               1|     150.0|10/16/19 16:19|742 14th St, San ...|\n",
      "|  259368|    27in FHD Monitor|               1|    149.99|10/13/19 13:47|960 Ridge St, Bos...|\n",
      "|  259369|USB-C Charging Cable|               1|     11.95|10/26/19 10:54|538 Johnson St, P...|\n",
      "|  259370|        20in Monitor|               1|    109.99|10/28/19 23:56|528 Forest St, Sa...|\n",
      "|  259371|              iPhone|               1|     700.0|10/26/19 12:12|306 Main St, Los ...|\n",
      "|  259372|    Wired Headphones|               1|     11.99|10/17/19 11:07|434 Madison St, S...|\n",
      "|  259373|    27in FHD Monitor|               1|    149.99|10/31/19 20:15|924 Church St, Sa...|\n",
      "|  259374|Apple Airpods Hea...|               1|     150.0|10/17/19 22:14|784 6th St, Los A...|\n",
      "|  259375|USB-C Charging Cable|               1|     11.95|10/10/19 08:53|557 Forest St, Sa...|\n",
      "|  259376|    27in FHD Monitor|               1|    149.99|10/12/19 00:03|218 14th St, San ...|\n",
      "|  259377|Bose SoundSport H...|               1|     99.99|10/23/19 16:49|88 Main St, Bosto...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_October_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_October_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_October_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b84a344-4dfb-45e7-b521-b3dd407103d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 20379\n"
     ]
    }
   ],
   "source": [
    "Sales_October_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_October_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c49f19cf-e052-4d5e-ab5c-4944334ec626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|summary|         Order ID|     Product|  Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|  count|            20284|       20317|             20284|             20284|         20317|               20317|\n",
      "|   mean|269078.5231216723|        null|1.1193551567738118| 183.1839400513232|          null|                null|\n",
      "| stddev|5612.651508571799|        null|0.4369219744825846|334.00512329766707|          null|                null|\n",
      "|    min|           259358|20in Monitor|                 1|              2.99|10/01/19 03:12|1 11th St, Los An...|\n",
      "|    max|           278796|      iPhone|                 8|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_October_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f741b487-fdfe-4e5e-8c4b-b68cd42d4cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 95\n",
      "Product: 62\n",
      "Quantity Ordered: 95\n",
      "Price Each: 95\n",
      "Order Date: 62\n",
      "Purchase Address: 62\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_October_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_October_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_October_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15aac2b6-d834-4abd-a7cd-887816c2fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': USB-C Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 11.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_October_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80401ded-d181-4a47-b684-d878f4f6a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_October_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_October_df = Sales_October_df.withColumn(col_name, when(Sales_October_df[col_name].isNull(), mean_val).otherwise(Sales_October_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ba7c80-02e9-42f2-abbe-de76941f3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_October_df = Sales_October_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27db4eb2-7bb9-44a4-aa85-da9afb07846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: USB-C Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_October_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8117a06-4f3c-4d06-9dec-e248cc5a5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_October_df = Sales_October_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e999b657-9163-47a8-8e97-60ccee25d135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.30%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_October_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_October_df.columns:\n",
    "    null_count = Sales_October_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aedd7ffc-aa8e-4a46-93cc-330ac7bf7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"13 Hill St, Atlanta\"\n",
    "Sales_October_df = Sales_October_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fcf89a1-9226-4d12-a7fc-8ce0c2316ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_October_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_October_df.columns:\n",
    "    null_count =Sales_October_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cac33f94-d208-4e1e-a6c1-c0cd7a68cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 20379\n",
      "After drop dublication: 20379\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_October_df.count())\n",
    "Sales_October_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_October_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357d222-de25-4072-ac2b-d9f8cd89e32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321315ff-3aab-4762-9e2c-fba4b845038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc75b36-1872-47ff-b1c5-9439a33e5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b923ca-3ce0-47d0-927b-2ed4b5baf262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  278797|    Wired Headphones|               1|     11.99|11/21/19 09:54|46 Park St, New Y...|\n",
      "|  278798|USB-C Charging Cable|               2|     11.95|11/17/19 10:03|962 Hickory St, A...|\n",
      "|  278799|Apple Airpods Hea...|               1|     150.0|11/19/19 14:56|464 Cherry St, Lo...|\n",
      "|  278800|    27in FHD Monitor|               1|    149.99|11/25/19 22:24|649 10th St, Seat...|\n",
      "|  278801|Bose SoundSport H...|               1|     99.99|11/09/19 13:56|522 Hill St, Bost...|\n",
      "|  278802|USB-C Charging Cable|               1|     11.95|11/14/19 20:34|154 2nd St, San F...|\n",
      "|  278803|Lightning Chargin...|               1|     14.95|11/11/19 08:05|724 5th St, San F...|\n",
      "|  278804|Bose SoundSport H...|               1|     99.99|11/15/19 11:48|866 Sunset St, Au...|\n",
      "|  278805|Lightning Chargin...|               1|     14.95|11/27/19 11:50|670 Elm St, San F...|\n",
      "|  278806|Bose SoundSport H...|               1|     99.99|11/19/19 19:12|174 2nd St, Bosto...|\n",
      "|  278806|     ThinkPad Laptop|               1|    999.99|11/19/19 19:12|174 2nd St, Bosto...|\n",
      "|  278807|     ThinkPad Laptop|               1|    999.99|11/25/19 21:52|240 Elm St, Austi...|\n",
      "|  278808|AAA Batteries (4-...|               1|      2.99|11/23/19 13:19|155 Highland St, ...|\n",
      "|  278809|AA Batteries (4-p...|               1|      3.84|11/17/19 12:38|969 13th St, Dall...|\n",
      "|  278810|USB-C Charging Cable|               1|     11.95|11/03/19 16:55|454 Cherry St, Bo...|\n",
      "|  278811|    Wired Headphones|               1|     11.99|11/06/19 16:01|99 Elm St, San Fr...|\n",
      "|  278811|Apple Airpods Hea...|               1|     150.0|11/06/19 16:01|99 Elm St, San Fr...|\n",
      "|  278812|Apple Airpods Hea...|               1|     150.0|11/17/19 20:08|601 Church St, Se...|\n",
      "|  278813|  Macbook Pro Laptop|               1|    1700.0|11/09/19 22:40|131 6th St, Bosto...|\n",
      "|  278814|AA Batteries (4-p...|               3|      3.84|11/16/19 16:45|419 Hill St, Bost...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_November_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_November_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_November_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c52703f-925a-43c4-a513-19f9649f574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 17661\n"
     ]
    }
   ],
   "source": [
    "Sales_November_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_November_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388526ad-6890-4eb4-bf2e-948ebd0e99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|  Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "|  count|             17580|       17616|             17580|             17580|         17616|               17616|\n",
      "|   mean|287235.96279863484|        null|1.1267349260523323|180.88196814565705|          null|                null|\n",
      "| stddev| 4866.884258246139|        null|0.4520109849640929| 330.1758948269367|          null|                null|\n",
      "|    min|            278797|20in Monitor|                 1|              2.99|11/01/19 03:18|1 12th St, San Fr...|\n",
      "|    max|            295664|      iPhone|                 8|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_November_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734b899a-91d0-4638-bd06-ae1d94714202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 81\n",
      "Product: 45\n",
      "Quantity Ordered: 81\n",
      "Price Each: 81\n",
      "Order Date: 45\n",
      "Purchase Address: 45\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_November_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_November_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_November_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd1bb50-fefa-4d83-b234-2c5672931cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': USB-C Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 11.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_November_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d39b8c-6e37-4213-bfd2-3c86a5042129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_November_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_November_df = Sales_November_df.withColumn(col_name, when(Sales_November_df[col_name].isNull(), mean_val).otherwise(Sales_November_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb6f2df-10a9-40cd-96cd-90d712c6c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_November_df = Sales_November_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1fb374-37f1-4912-abd1-82565fc14be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: USB-C Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_November_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62254cf9-3365-44db-b9c4-f00aa6da59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_November_df = Sales_November_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2717c221-ee30-4b89-a70f-6454fcffaf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.25%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_November_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_November_df.columns:\n",
    "    null_count = Sales_November_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7478ef6-0ed2-4e02-bc04-755072e6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"802 Jefferson St, San Francisco\"\n",
    "Sales_November_df = Sales_November_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab2f15e-a43c-4024-bff5-363d44aaea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_November_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_November_df.columns:\n",
    "    null_count = Sales_November_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e97404-9977-4f46-8dab-31a30b0569ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 17661\n",
      "After drop dublication: 17661\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_November_df.count())\n",
    "Sales_November_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_November_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5b7bc-7cb7-4c73-8a7e-a28ff8d5b328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74aa19f-cdb4-4a60-ab0b-4560bb25a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfa27b0-d32f-4af7-b868-9c854ec9f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"20g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22ccc42-4bd9-411f-883f-073ad3b32a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  295665|  Macbook Pro Laptop|               1|    1700.0|12/30/19 00:01|136 Church St, Ne...|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|12/29/19 07:03|562 2nd St, New Y...|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|12/12/19 18:21|277 Main St, New ...|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|12/22/19 15:13|410 6th St, San F...|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|12/18/19 12:38|43 Hill St, Atlan...|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|12/31/19 22:58|200 Jefferson St,...|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|12/16/19 15:10|928 12th St, Port...|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|12/13/19 09:29|813 Hickory St, D...|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|12/15/19 23:26|718 Wilson St, Da...|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|12/28/19 11:51|77 7th St, Dallas...|\n",
      "|  295675|USB-C Charging Cable|               2|     11.95|12/13/19 13:52|594 1st St, San F...|\n",
      "|  295676|     ThinkPad Laptop|               1|    999.99|12/28/19 17:19|410 Lincoln St, L...|\n",
      "|  295677|AA Batteries (4-p...|               2|      3.84|12/20/19 19:19|866 Pine St, Bost...|\n",
      "|  295678|AAA Batteries (4-...|               2|      2.99|12/06/19 09:38|187 Lincoln St, D...|\n",
      "|  295679|USB-C Charging Cable|               1|     11.95|12/25/19 09:39|902 2nd St, Dalla...|\n",
      "|  295680|Lightning Chargin...|               1|     14.95|12/01/19 14:30|338 Main St, Aust...|\n",
      "|  295681|        Google Phone|               1|     600.0|12/25/19 12:37|79 Elm St, Boston...|\n",
      "|  295681|USB-C Charging Cable|               1|     11.95|12/25/19 12:37|79 Elm St, Boston...|\n",
      "|  295681|Bose SoundSport H...|               1|     99.99|12/25/19 12:37|79 Elm St, Boston...|\n",
      "|  295681|    Wired Headphones|               1|     11.99|12/25/19 12:37|79 Elm St, Boston...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/C:\\Users\\Reem\\Downloads\\archive\\Sales_Data\\Sales_December_2019.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "Sales_December_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "Sales_December_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105145cc-0778-4858-bb18-1d8d53ef8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 25117\n"
     ]
    }
   ],
   "source": [
    "Sales_December_df.printSchema()\n",
    "print(\"Count of dataframe:\",Sales_December_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a29f72-cf42-4d90-b378-3eecfe45e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|         Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|            24989|       25037|              24989|             24989|         25037|               25037|\n",
      "|   mean|307655.0231701949|        null| 1.1253351474648845|183.84565008610213|          null|                null|\n",
      "| stddev|6932.795455986216|        null|0.44541356230082607| 333.0770368580182|          null|                null|\n",
      "|    min|           295665|20in Monitor|                  1|              2.99|01/01/20 00:10|1 12th St, San Fr...|\n",
      "|    max|           319670|      iPhone|                  7|            1700.0|    Order Date|    Purchase Address|\n",
      "+-------+-----------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_December_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2306e97e-69c2-4977-9660-c58cf6cdcc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "Order ID: 128\n",
      "Product: 80\n",
      "Quantity Ordered: 128\n",
      "Price Each: 128\n",
      "Order Date: 80\n",
      "Purchase Address: 80\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = Sales_December_df.agg(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c + '_null_count') for c in Sales_December_df.columns])\n",
    "\n",
    "# Collect the result as a single row\n",
    "null_counts_single_row = null_counts.collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of null values in each column:\")\n",
    "for col_name in Sales_December_df.columns:\n",
    "    print(f\"{col_name}: {null_counts_single_row[col_name + '_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f04629c-4d60-449a-bd04-a45d03c1e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value in column 'Order ID': None\n",
      "Most frequent value in column 'Product': USB-C Charging Cable\n",
      "Most frequent value in column 'Quantity Ordered': 1\n",
      "Most frequent value in column 'Price Each': 11.95\n",
      "Most frequent value in column 'Order Date': None\n",
      "Most frequent value in column 'Purchase Address': None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to find most frequent values\n",
    "cols_to_check = [\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Find the most frequent value in each column\n",
    "most_frequent_values = []\n",
    "for col_name in cols_to_check:\n",
    "    mode_value = Sales_December_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).select(col_name).first()[0]\n",
    "    most_frequent_values.append((col_name, mode_value))\n",
    "\n",
    "# Print the most frequent value in each column\n",
    "for col_name, value in most_frequent_values:\n",
    "    print(f\"Most frequent value in column '{col_name}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4392ac23-826d-4fb4-b6f5-df813bdbba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when\n",
    "\n",
    "# Example: Mode imputation for categorical columns\n",
    "categorical_cols = [\"Order ID\", \"Quantity Ordered\", \"Price Each\"]\n",
    "\n",
    "\n",
    "# Example: Mean imputation for numerical columns\n",
    "numerical_cols =[\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Order Date\", \"Purchase Address\"]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = {}\n",
    "for col_name in numerical_cols:\n",
    "    mean_val = Sales_December_df.agg({col_name: 'mean'}).collect()[0][f'avg({col_name})']\n",
    "    mean_values[col_name] = mean_val\n",
    "\n",
    "# Fill null values with mean\n",
    "for col_name, mean_val in mean_values.items():\n",
    "    Sales_December_df = Sales_December_df.withColumn(col_name, when(Sales_December_df[col_name].isNull(), mean_val).otherwise(Sales_December_df[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941aa7a3-b0ad-4cd7-a598-3010b21b241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_December_df = Sales_December_df.drop('Order Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d48a6a-9ce3-4b22-bfcc-67b6d511f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: USB-C Charging Cable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by the column and count occurrences, then order by count\n",
    "most_frequent_value = Sales_December_df.groupBy(\"Product\") \\\n",
    "                        .count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .select(\"Product\") \\\n",
    "                        .first()[0]\n",
    "\n",
    "print(\"Most frequent value:\", most_frequent_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ec89ce-83b8-4c45-9df8-3aa9abc70dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_December_df = Sales_December_df.fillna(most_frequent_value, subset=[\"Product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e659e25-24bf-4b9a-8410-859368caa71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.32%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_December_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_December_df.columns:\n",
    "    null_count = Sales_December_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d45a85f-a2f4-49ae-87f7-8029afa0cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = \"893 Elm St, Los Angeles\"\n",
    "Sales_December_df = Sales_December_df.fillna(default_value, subset=['Purchase Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb7514b9-d2c8-4c21-9bf7-db75a11f18e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage in column 'Order ID': 0.00%\n",
      "Null percentage in column 'Product': 0.00%\n",
      "Null percentage in column 'Quantity Ordered': 0.00%\n",
      "Null percentage in column 'Price Each': 0.00%\n",
      "Null percentage in column 'Purchase Address': 0.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "total_rows = Sales_December_df.count()\n",
    "null_percentages = []\n",
    "for col_name in Sales_December_df.columns:\n",
    "    null_count = Sales_December_df.where(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_percentages.append((col_name, null_percentage))\n",
    "\n",
    "# Print null percentages\n",
    "for col_name, percentage in null_percentages:\n",
    "    print(f\"Null percentage in column '{col_name}': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee2a99b-eb8a-41eb-8976-0fccbce4d66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop dublication: 25117\n",
      "After drop dublication: 25117\n"
     ]
    }
   ],
   "source": [
    "print(\"Before drop dublication:\",Sales_December_df.count())\n",
    "Sales_December_df.dropDuplicates()\n",
    "print(\"After drop dublication:\",Sales_December_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd1245-7f35-4f8b-a267-f0df6f7395c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
