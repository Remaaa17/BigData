{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4975987-9b1f-4e3f-9e68-6feab42c6c63",
   "metadata": {},
   "source": [
    "## import library and load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7aebda9-3a1b-4177-b780-43bd76b6c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ee508d-7187-485c-8b87-be21610d881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Spark_Project\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"10g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec88432-fc56-40fc-9efe-a6ac29bfba68",
   "metadata": {},
   "source": [
    "* show customers dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb119bf-6d3d-4bae-bc3a-ae325beb3e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "|customer_id|      customer_name|     gender|age|        home_address|zip_code|               city|               state|  country|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "|          1|      Leanna Busson|     Female| 30|8606 Victoria Ter...|    5464|      Johnstonhaven|  Northern Territory|Australia|\n",
      "|          2|Zabrina Harrowsmith|Genderfluid| 69|8327 Kirlin Summi...|    8223|    New Zacharyfort|     South Australia|Australia|\n",
      "|          3|    Shina Dullaghan| Polygender| 59|269 Gemma SummitS...|    5661|           Aliburgh|Australian Capita...|Australia|\n",
      "|          4|      Hewet McVitie|   Bigender| 67|743 Bailey GroveS...|    1729|  South Justinhaven|          Queensland|Australia|\n",
      "|          5|     Rubia Ashleigh| Polygender| 30|48 Hyatt ManorSui...|    4032|     Griffithsshire|          Queensland|Australia|\n",
      "|          6|     Cordey Tolcher|Genderfluid| 40|7118 Mccullough S...|    9996|         Blakehaven|     New South Wales|Australia|\n",
      "|          7|    Winslow Ewbanck|   Bigender| 76|92 Hills Station ...|     793|          Masonfurt|          Queensland|Australia|\n",
      "|          8|       Marlowe Wynn|    Agender| 75|383 Muller Summit...|    7681|            Samside|  Northern Territory|Australia|\n",
      "|          9|  Brittaney Gontier|       Male| 51|57 Greenfelder Hi...|       2|          Beierport|  Northern Territory|Australia|\n",
      "|         10|  Susanetta Wilshin|   Bigender| 70|615 Hayley KnollS...|    2118|          Joelburgh|   Western Australia|Australia|\n",
      "|         11|Michaeline McIndrew|    Agender| 39|96 Daniel PlaceSu...|    7400|         Georgeland|  Northern Territory|Australia|\n",
      "|         12|       Fedora Dmych|   Bigender| 78|66 Kayla MewsSuit...|    6334|        Taylorburgh|     South Australia|Australia|\n",
      "|         13|    Marabel Swinfon|       Male| 42|16 Kuhn LoopSuite...|    6170|      Maddisonmouth|          Queensland|Australia|\n",
      "|         14|    Chrissie Wackly|   Bigender| 36|403 Doherty RunSu...|    2050|North Benjaminville|     New South Wales|Australia|\n",
      "|         15|     Avril Rossiter|Genderfluid| 34|254 Ali RidgeApt....|     491|         Kiehnburgh|   Western Australia|Australia|\n",
      "|         16|  Gabbie Aldwinckle|       Male| 75|424 Mason PlaceAp...|    6438|            New Kai|            Victoria|Australia|\n",
      "|         17|    Devonna Cutting|Genderfluid| 32|29 Imogen CrestSu...|    8309|     Lake Graceside|Australian Capita...|Australia|\n",
      "|         18|      Chan Duchesne|       Male| 79|13 Bailey ManorAp...|    7171|         Walterland|     New South Wales|Australia|\n",
      "|         19|   Chadwick Cruddas|    Agender| 41|002 Beau PlazaApt...|    7053|        West Bailey|            Victoria|Australia|\n",
      "|         20|       Gigi Kalaher|Genderqueer| 55|2069 Phoebe MallA...|    6318|        East Audrey|     South Australia|Australia|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\customers.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "df_customers = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_customers .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b935d5-bceb-44d9-a0c7-52d80adb682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- home_address: string (nullable = true)\n",
      " |-- zip_code: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 1000\n"
     ]
    }
   ],
   "source": [
    "df_customers .printSchema()\n",
    "print(\"Count of dataframe:\",df_customers.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d81768-12c0-4845-956b-f71b1eaa01b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "|summary|      customer_id| customer_name|    gender|               age|        home_address|         zip_code|     city|               state|  country|\n",
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "|  count|             1000|          1000|      1000|              1000|                1000|             1000|     1000|                1000|     1000|\n",
      "|   mean|            500.5|          null|      null|             49.86|                null|         5004.872|     null|                null|     null|\n",
      "| stddev|288.8194360957494|          null|      null|17.647828360618387|                null|2884.497332027621|     null|                null|     null|\n",
      "|    min|                1|Abbot Rickaert|   Agender|                20|00 Fadel CircuitA...|                2|Aaronbury|Australian Capita...|Australia|\n",
      "|    max|             1000|   Zulema Teml|Polygender|                80|9993 Wood RidgeAp...|             9998| Zacville|   Western Australia|Australia|\n",
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313572ef-f7b3-4c02-a7d3-a0552a5516f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|               age|         zip_code|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|              1000|             1000|\n",
      "|   mean|             49.86|         5004.872|\n",
      "| stddev|17.647828360618387|2884.497332027621|\n",
      "|    min|                20|                2|\n",
      "|    max|                80|             9998|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.describe([\"age\", \"zip_code\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c5ce3-3676-4740-8f97-26ba028d0402",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308cee0-d2f4-4849-a38b-4ce65bfae7d8",
   "metadata": {},
   "source": [
    "#### Cleanning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2f0cb5-c285-4297-a349-aeafffff2e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "|customer_id|      customer_name|     gender|age|        home_address|zip_code|               city|               state|  country|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "|          1|      Leanna Busson|     Female| 30|8606 Victoria Ter...|    5464|      Johnstonhaven|  Northern Territory|Australia|\n",
      "|          2|Zabrina Harrowsmith|Genderfluid| 69|8327 Kirlin Summi...|    8223|    New Zacharyfort|     South Australia|Australia|\n",
      "|          3|    Shina Dullaghan| Polygender| 59|269 Gemma SummitS...|    5661|           Aliburgh|Australian Capita...|Australia|\n",
      "|          4|      Hewet McVitie|   Bigender| 67|743 Bailey GroveS...|    1729|  South Justinhaven|          Queensland|Australia|\n",
      "|          5|     Rubia Ashleigh| Polygender| 30|48 Hyatt ManorSui...|    4032|     Griffithsshire|          Queensland|Australia|\n",
      "|          6|     Cordey Tolcher|Genderfluid| 40|7118 Mccullough S...|    9996|         Blakehaven|     New South Wales|Australia|\n",
      "|          7|    Winslow Ewbanck|   Bigender| 76|92 Hills Station ...|     793|          Masonfurt|          Queensland|Australia|\n",
      "|          8|       Marlowe Wynn|    Agender| 75|383 Muller Summit...|    7681|            Samside|  Northern Territory|Australia|\n",
      "|          9|  Brittaney Gontier|       Male| 51|57 Greenfelder Hi...|       2|          Beierport|  Northern Territory|Australia|\n",
      "|         10|  Susanetta Wilshin|   Bigender| 70|615 Hayley KnollS...|    2118|          Joelburgh|   Western Australia|Australia|\n",
      "|         11|Michaeline McIndrew|    Agender| 39|96 Daniel PlaceSu...|    7400|         Georgeland|  Northern Territory|Australia|\n",
      "|         12|       Fedora Dmych|   Bigender| 78|66 Kayla MewsSuit...|    6334|        Taylorburgh|     South Australia|Australia|\n",
      "|         13|    Marabel Swinfon|       Male| 42|16 Kuhn LoopSuite...|    6170|      Maddisonmouth|          Queensland|Australia|\n",
      "|         14|    Chrissie Wackly|   Bigender| 36|403 Doherty RunSu...|    2050|North Benjaminville|     New South Wales|Australia|\n",
      "|         15|     Avril Rossiter|Genderfluid| 34|254 Ali RidgeApt....|     491|         Kiehnburgh|   Western Australia|Australia|\n",
      "|         16|  Gabbie Aldwinckle|       Male| 75|424 Mason PlaceAp...|    6438|            New Kai|            Victoria|Australia|\n",
      "|         17|    Devonna Cutting|Genderfluid| 32|29 Imogen CrestSu...|    8309|     Lake Graceside|Australian Capita...|Australia|\n",
      "|         18|      Chan Duchesne|       Male| 79|13 Bailey ManorAp...|    7171|         Walterland|     New South Wales|Australia|\n",
      "|         19|   Chadwick Cruddas|    Agender| 41|002 Beau PlazaApt...|    7053|        West Bailey|            Victoria|Australia|\n",
      "|         20|       Gigi Kalaher|Genderqueer| 55|2069 Phoebe MallA...|    6318|        East Audrey|     South Australia|Australia|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e605887-6239-41d8-8730-a3ae00f5f446",
   "metadata": {},
   "source": [
    "> here we will check if there null values from our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf31ef0-7346-4e4b-bb9e-cc7fc07914ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers=df_customers.dropna()\n",
    "df_customers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166b6574-cc69-4dfb-b5e3-f91a4ceb2765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "|customer_id|      customer_name|gender|age|        home_address|zip_code|               city|               state|  country|\n",
      "+-----------+-------------------+------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "|          1|      Leanna Busson|Female| 30|8606 Victoria Ter...|    5464|      Johnstonhaven|  Northern Territory|Australia|\n",
      "|          2|Zabrina Harrowsmith| Other| 69|8327 Kirlin Summi...|    8223|    New Zacharyfort|     South Australia|Australia|\n",
      "|          3|    Shina Dullaghan| Other| 59|269 Gemma SummitS...|    5661|           Aliburgh|Australian Capita...|Australia|\n",
      "|          4|      Hewet McVitie| Other| 67|743 Bailey GroveS...|    1729|  South Justinhaven|          Queensland|Australia|\n",
      "|          5|     Rubia Ashleigh| Other| 30|48 Hyatt ManorSui...|    4032|     Griffithsshire|          Queensland|Australia|\n",
      "|          6|     Cordey Tolcher| Other| 40|7118 Mccullough S...|    9996|         Blakehaven|     New South Wales|Australia|\n",
      "|          7|    Winslow Ewbanck| Other| 76|92 Hills Station ...|     793|          Masonfurt|          Queensland|Australia|\n",
      "|          8|       Marlowe Wynn| Other| 75|383 Muller Summit...|    7681|            Samside|  Northern Territory|Australia|\n",
      "|          9|  Brittaney Gontier|  Male| 51|57 Greenfelder Hi...|       2|          Beierport|  Northern Territory|Australia|\n",
      "|         10|  Susanetta Wilshin| Other| 70|615 Hayley KnollS...|    2118|          Joelburgh|   Western Australia|Australia|\n",
      "|         11|Michaeline McIndrew| Other| 39|96 Daniel PlaceSu...|    7400|         Georgeland|  Northern Territory|Australia|\n",
      "|         12|       Fedora Dmych| Other| 78|66 Kayla MewsSuit...|    6334|        Taylorburgh|     South Australia|Australia|\n",
      "|         13|    Marabel Swinfon|  Male| 42|16 Kuhn LoopSuite...|    6170|      Maddisonmouth|          Queensland|Australia|\n",
      "|         14|    Chrissie Wackly| Other| 36|403 Doherty RunSu...|    2050|North Benjaminville|     New South Wales|Australia|\n",
      "|         15|     Avril Rossiter| Other| 34|254 Ali RidgeApt....|     491|         Kiehnburgh|   Western Australia|Australia|\n",
      "|         16|  Gabbie Aldwinckle|  Male| 75|424 Mason PlaceAp...|    6438|            New Kai|            Victoria|Australia|\n",
      "|         17|    Devonna Cutting| Other| 32|29 Imogen CrestSu...|    8309|     Lake Graceside|Australian Capita...|Australia|\n",
      "|         18|      Chan Duchesne|  Male| 79|13 Bailey ManorAp...|    7171|         Walterland|     New South Wales|Australia|\n",
      "|         19|   Chadwick Cruddas| Other| 41|002 Beau PlazaApt...|    7053|        West Bailey|            Victoria|Australia|\n",
      "|         20|       Gigi Kalaher| Other| 55|2069 Phoebe MallA...|    6318|        East Audrey|     South Australia|Australia|\n",
      "+-----------+-------------------+------+---+--------------------+--------+-------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Replace values in 'gender' column directly\n",
    "df_customers = df_customers.withColumn(\n",
    "    'gender',\n",
    "    when(df_customers['gender'].isin(['Male', 'Female']), df_customers['gender']).otherwise('Other')\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_customers.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d1932-a4d6-4e13-8f26-2d6e90d6ff32",
   "metadata": {},
   "source": [
    "> here we will check if there Duplicates or outliers in our dataset  and if there will handle it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c3e29c-bd15-43ce-89a0-5b515d605aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|customer_id|       customer_name|gender|age|        home_address|zip_code|                city|               state|  country|\n",
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|        152|     Panchito Wybern|Female| 79|03 Glover TrackAp...|    7845|       Christianport|Australian Capita...|Australia|\n",
      "|        239|      Daria Embleton| Other| 63|39 Wilkinson Cres...|    1644|        North Sophie|Australian Capita...|Australia|\n",
      "|        367|Hildagarde Dell '...| Other| 51|59 Henry PlaceSui...|     514|          Jasperside|     South Australia|Australia|\n",
      "|        399|     Ailene McKeller| Other| 46|9444 Casper Ridge...|    8209|         South Logan|   Western Australia|Australia|\n",
      "|        446|       Benny Songist| Other| 47|165 Hermiston Loo...|    1552|     North Lukeshire|            Tasmania|Australia|\n",
      "|        613|   Melamie Craigmyle| Other| 78|2525 Leannon Junc...|    6475|           Stellaton|     New South Wales|Australia|\n",
      "|        903|    Leonhard Webland|  Male| 48|9694 Nathan Trail...|    2370|          Walkerfurt|     New South Wales|Australia|\n",
      "|        941|    Egon Figliovanni|Female| 59|617 Morrison Cres...|     105|      Farrellchester|     New South Wales|Australia|\n",
      "|         56|        Vinny Niland| Other| 33|673 Amelie MewsAp...|    6829|          Brownburgh|     New South Wales|Australia|\n",
      "|        278|      Michele Osmant| Other| 73|370 Koch SummitAp...|    8967|   North Josephmouth|          Queensland|Australia|\n",
      "|        484| Padraig Jouannisson|Female| 43|792 James PlaceSu...|    7271|South Cameronborough|Australian Capita...|Australia|\n",
      "|        915|    Glenine Saladine|  Male| 70|9738 Liam RoadApt...|    9647|            Lukefort|Australian Capita...|Australia|\n",
      "|        281|       Karla Teresia| Other| 24|95 Oliver SquareA...|    7637|             Evafort|          Queensland|Australia|\n",
      "|        525|      Melisse Hopfer| Other| 28|857 Mayer SquareA...|    5639|         Port Ashley|     New South Wales|Australia|\n",
      "|        595|    Thomasine Baudet|Female| 60|313 Grace LoopApt...|     939|          Ameliaview|     South Australia|Australia|\n",
      "|        895|      Robena Gilbert| Other| 37|6873 Berge Street...|    6795|     Lake Bellahaven|          Queensland|Australia|\n",
      "|        919|        Belia Hovard| Other| 64|443 Jesse MewsApt...|      24|          New Imogen|          Queensland|Australia|\n",
      "|        217|     Silvan Norcross| Other| 29|476 Ferry IslandS...|     742|       South Georgia|Australian Capita...|Australia|\n",
      "|        331|     Binnie Gonzales| Other| 32|5756 Savannah Dri...|    7980|            Aidenton|  Northern Territory|Australia|\n",
      "|        582|  Amberly Seckington|Female| 65|5260 Ward IslandA...|     429|            Skyeland|     South Australia|Australia|\n",
      "|        683|    Esteban Hensmans| Other| 45|098 Clarke Parkwa...|    3661|       West Finnberg|Australian Capita...|Australia|\n",
      "|        758|    Mallissa Lawdham| Other| 44|96 Bode RidgeApt....|     697|        East Madison|     New South Wales|Australia|\n",
      "|        393|       Anni Heinsius| Other| 34|5158 Levi HillSui...|    1474|          Johnsburgh|          Queensland|Australia|\n",
      "|        648|        Jaquelyn Dow| Other| 38|854 John GroveApt...|    1230|        New Emmastad|     South Australia|Australia|\n",
      "|         46|        Meris Twitty| Other| 68|708 Daniel Meadow...|    5087|      Bartolettiside|Australian Capita...|Australia|\n",
      "|        127|       Merl Gayforth| Other| 30|769 Gibson TrailA...|    3708|          South Adam|   Western Australia|Australia|\n",
      "|        350|         Dani Hawtin| Other| 23|41 Brown Estate D...|    4913|          North Alex|            Victoria|Australia|\n",
      "|        392|       Svend Griffey| Other| 26|992 Henry Station...|    6749|          Gloverbury|Australian Capita...|Australia|\n",
      "|        410|      Harman Finding| Other| 64|77 Graham AvenueS...|    7356|          Boyerville|            Victoria|Australia|\n",
      "|        507|       Brooks Sayers| Other| 74|64 Champlin Parkw...|    8416|          Jaydenside|            Victoria|Australia|\n",
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Handling Duplicates\n",
    "df_customers =df_customers.dropDuplicates()\n",
    "# Handling Outliers \n",
    "# Assuming outliers are defined as age < 0 or age > 100\n",
    "df_customers =df_customers.filter((col(\"age\") >= 0) & (col(\"age\") <= 100))\n",
    "df_customers.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26162bc-7a42-4d53-8968-cca2faed100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|customer_id|       customer_name|gender|age|        home_address|zip_code|                city|               state|  country|\n",
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|        152|     Panchito Wybern|Female| 79|03 Glover TrackAp...|    7845|       Christianport|Australian Capita...|Australia|\n",
      "|        239|      Daria Embleton| Other| 63|39 Wilkinson Cres...|    1644|        North Sophie|Australian Capita...|Australia|\n",
      "|        367|Hildagarde Dell '...| Other| 51|59 Henry PlaceSui...|     514|          Jasperside|     South Australia|Australia|\n",
      "|        399|     Ailene McKeller| Other| 46|9444 Casper Ridge...|    8209|         South Logan|   Western Australia|Australia|\n",
      "|        446|       Benny Songist| Other| 47|165 Hermiston Loo...|    1552|     North Lukeshire|            Tasmania|Australia|\n",
      "|        613|   Melamie Craigmyle| Other| 78|2525 Leannon Junc...|    6475|           Stellaton|     New South Wales|Australia|\n",
      "|        903|    Leonhard Webland|  Male| 48|9694 Nathan Trail...|    2370|          Walkerfurt|     New South Wales|Australia|\n",
      "|        941|    Egon Figliovanni|Female| 59|617 Morrison Cres...|     105|      Farrellchester|     New South Wales|Australia|\n",
      "|         56|        Vinny Niland| Other| 33|673 Amelie MewsAp...|    6829|          Brownburgh|     New South Wales|Australia|\n",
      "|        278|      Michele Osmant| Other| 73|370 Koch SummitAp...|    8967|   North Josephmouth|          Queensland|Australia|\n",
      "|        484| Padraig Jouannisson|Female| 43|792 James PlaceSu...|    7271|South Cameronborough|Australian Capita...|Australia|\n",
      "|        915|    Glenine Saladine|  Male| 70|9738 Liam RoadApt...|    9647|            Lukefort|Australian Capita...|Australia|\n",
      "|        281|       Karla Teresia| Other| 24|95 Oliver SquareA...|    7637|             Evafort|          Queensland|Australia|\n",
      "|        525|      Melisse Hopfer| Other| 28|857 Mayer SquareA...|    5639|         Port Ashley|     New South Wales|Australia|\n",
      "|        595|    Thomasine Baudet|Female| 60|313 Grace LoopApt...|     939|          Ameliaview|     South Australia|Australia|\n",
      "|        895|      Robena Gilbert| Other| 37|6873 Berge Street...|    6795|     Lake Bellahaven|          Queensland|Australia|\n",
      "|        919|        Belia Hovard| Other| 64|443 Jesse MewsApt...|      24|          New Imogen|          Queensland|Australia|\n",
      "|        217|     Silvan Norcross| Other| 29|476 Ferry IslandS...|     742|       South Georgia|Australian Capita...|Australia|\n",
      "|        331|     Binnie Gonzales| Other| 32|5756 Savannah Dri...|    7980|            Aidenton|  Northern Territory|Australia|\n",
      "|        582|  Amberly Seckington|Female| 65|5260 Ward IslandA...|     429|            Skyeland|     South Australia|Australia|\n",
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers = df_customers.drop('new_gender')\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0dc61c2-6f77-45e8-a345-4e62196d30dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female|  115|\n",
      "| Other|  742|\n",
      "|  Male|  143|\n",
      "+------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|  country|count|\n",
      "+---------+-----+\n",
      "|Australia| 1000|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.groupBy(\"gender\").count().show()\n",
    "df_customers.groupBy(\"country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2efc6a0-88b3-44d4-98d4-06d2d911574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|unique_states|\n",
      "+-------------+\n",
      "|            8|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|unique_cities|\n",
      "+-------------+\n",
      "|          961|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Count the number of unique states\n",
    "unique_states_df = df_customers.agg(F.countDistinct(\"state\").alias(\"unique_states\"))\n",
    "unique_states_df.show()\n",
    "# Count the number of unique cities\n",
    "unique_cities_df = df_customers.agg(F.countDistinct(\"city\").alias(\"unique_cities\"))\n",
    "unique_cities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9fbd3b-adc7-4df7-a7ef-e02c091ad3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|average_age|\n",
      "+-----------+\n",
      "|      49.86|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Calculate the average age of customers\n",
    "avg_age_df = df_customers.agg(F.avg(\"age\").alias(\"average_age\"))\n",
    "avg_age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d99d679-d7d9-4c05-b59f-259277556d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+\n",
      "|customer_id|       customer_name|gender|age|        home_address|zip_code|                city|               state|\n",
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+\n",
      "|        152|     Panchito Wybern|Female| 79|03 Glover TrackAp...|    7845|       Christianport|Australian Capita...|\n",
      "|        239|      Daria Embleton| Other| 63|39 Wilkinson Cres...|    1644|        North Sophie|Australian Capita...|\n",
      "|        367|Hildagarde Dell '...| Other| 51|59 Henry PlaceSui...|     514|          Jasperside|     South Australia|\n",
      "|        399|     Ailene McKeller| Other| 46|9444 Casper Ridge...|    8209|         South Logan|   Western Australia|\n",
      "|        446|       Benny Songist| Other| 47|165 Hermiston Loo...|    1552|     North Lukeshire|            Tasmania|\n",
      "|        613|   Melamie Craigmyle| Other| 78|2525 Leannon Junc...|    6475|           Stellaton|     New South Wales|\n",
      "|        903|    Leonhard Webland|  Male| 48|9694 Nathan Trail...|    2370|          Walkerfurt|     New South Wales|\n",
      "|        941|    Egon Figliovanni|Female| 59|617 Morrison Cres...|     105|      Farrellchester|     New South Wales|\n",
      "|         56|        Vinny Niland| Other| 33|673 Amelie MewsAp...|    6829|          Brownburgh|     New South Wales|\n",
      "|        278|      Michele Osmant| Other| 73|370 Koch SummitAp...|    8967|   North Josephmouth|          Queensland|\n",
      "|        484| Padraig Jouannisson|Female| 43|792 James PlaceSu...|    7271|South Cameronborough|Australian Capita...|\n",
      "|        915|    Glenine Saladine|  Male| 70|9738 Liam RoadApt...|    9647|            Lukefort|Australian Capita...|\n",
      "|        281|       Karla Teresia| Other| 24|95 Oliver SquareA...|    7637|             Evafort|          Queensland|\n",
      "|        525|      Melisse Hopfer| Other| 28|857 Mayer SquareA...|    5639|         Port Ashley|     New South Wales|\n",
      "|        595|    Thomasine Baudet|Female| 60|313 Grace LoopApt...|     939|          Ameliaview|     South Australia|\n",
      "|        895|      Robena Gilbert| Other| 37|6873 Berge Street...|    6795|     Lake Bellahaven|          Queensland|\n",
      "|        919|        Belia Hovard| Other| 64|443 Jesse MewsApt...|      24|          New Imogen|          Queensland|\n",
      "|        217|     Silvan Norcross| Other| 29|476 Ferry IslandS...|     742|       South Georgia|Australian Capita...|\n",
      "|        331|     Binnie Gonzales| Other| 32|5756 Savannah Dri...|    7980|            Aidenton|  Northern Territory|\n",
      "|        582|  Amberly Seckington|Female| 65|5260 Ward IslandA...|     429|            Skyeland|     South Australia|\n",
      "+-----------+--------------------+------+---+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers = df_customers.drop('country')\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd6f89-251e-43c1-8614-78f03f469204",
   "metadata": {},
   "source": [
    "\r\n",
    "**What are the counts of unique customers per state, along with the average age of customers in each state?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fee505e7-96ec-452a-947c-97f12f6c037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------------------+\n",
      "|               state|customer_count|           avg_age|\n",
      "+--------------------+--------------+------------------+\n",
      "|  Northern Territory|           125|            49.168|\n",
      "|     South Australia|           139| 49.86330935251799|\n",
      "|            Victoria|           121| 47.83471074380165|\n",
      "|   Western Australia|           124| 48.70161290322581|\n",
      "|     New South Wales|           132| 52.88636363636363|\n",
      "|Australian Capita...|           121|50.710743801652896|\n",
      "|            Tasmania|           104| 48.59615384615385|\n",
      "|          Queensland|           134|50.634328358208954|\n",
      "+--------------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import avg\n",
    "country_stats = df_customers.groupBy('state').agg(\n",
    "    countDistinct('customer_id').alias('customer_count'),\n",
    "    avg('age').alias('avg_age')\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "country_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979c04e-e832-43cf-a298-86c52b7edb24",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "**Which state has the highest number of customers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63958191-4bf7-4c19-a774-a9d7fd8cbd1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|               state|customer_count|\n",
      "+--------------------+--------------+\n",
      "|     South Australia|           139|\n",
      "|          Queensland|           134|\n",
      "|     New South Wales|           132|\n",
      "|  Northern Territory|           125|\n",
      "|   Western Australia|           124|\n",
      "|            Victoria|           121|\n",
      "|Australian Capita...|           121|\n",
      "|            Tasmania|           104|\n",
      "+--------------------+--------------+\n",
      "\n",
      "State with the most customers: South Australia (139 customers)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "# Group by city and count the number of customers in each city\n",
    "city_customer_count = df_customers.groupBy(\"state\").agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "# Sort cities by customer count in descending order\n",
    "sorted_cities = city_customer_count.orderBy(F.desc(\"customer_count\"))\n",
    "sorted_cities.show()\n",
    "most_customers_city = sorted_cities.select(\"state\", \"customer_count\").first()\n",
    "# Show the city with the most customer count\n",
    "print(f\"State with the most customers: {most_customers_city['state']} ({most_customers_city['customer_count']} customers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76285cc-0f06-4e73-bfa3-2cf30e51626f",
   "metadata": {},
   "source": [
    "* show orders dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba4ea64-152c-4ddd-a08b-d1411685c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+-------------------+-------------+\n",
      "|order_id|customer_id|payment|         order_date|delivery_date|\n",
      "+--------+-----------+-------+-------------------+-------------+\n",
      "|       1|         64|  30811|2021-08-30 00:00:00|   2021-09-24|\n",
      "|       2|        473|  50490|2021-02-03 00:00:00|   2021-02-13|\n",
      "|       3|        774|  46763|2021-10-08 00:00:00|   2021-11-03|\n",
      "|       4|        433|  39782|2021-05-06 00:00:00|   2021-05-19|\n",
      "|       5|        441|  14719|2021-03-23 00:00:00|   2021-03-24|\n",
      "|       6|        800|  16197|2021-09-09 00:00:00|   2021-10-05|\n",
      "|       7|        626|  37666|2021-04-05 00:00:00|   2021-04-11|\n",
      "|       8|         58|  28484|2021-04-12 00:00:00|   2021-05-01|\n",
      "|       9|        852|  12896|2021-05-01 00:00:00|   2021-05-11|\n",
      "|      10|        659|  21922|2021-10-15 00:00:00|   2021-10-16|\n",
      "|      11|        785|  36624|2021-06-15 00:00:00|   2021-06-30|\n",
      "|      12|        120|  55507|2021-06-30 00:00:00|   2021-07-11|\n",
      "|      13|        204|  57810|2021-10-15 00:00:00|   2021-10-31|\n",
      "|      14|        957|  21270|2021-03-29 00:00:00|   2021-04-12|\n",
      "|      15|        468|  15488|2021-09-22 00:00:00|   2021-10-08|\n",
      "|      16|        564|  36479|2021-07-07 00:00:00|   2021-07-08|\n",
      "|      17|        614|  51822|2021-02-09 00:00:00|   2021-02-11|\n",
      "|      18|        299|  22902|2021-01-01 00:00:00|   2021-01-03|\n",
      "|      19|        513|  56754|2021-08-30 00:00:00|   2021-09-10|\n",
      "|      20|        789|  45129|2021-01-13 00:00:00|   2021-01-20|\n",
      "+--------+-----------+-------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\orders.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "df_orders = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34460803-c49a-4f30-a766-1394a103bb08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- delivery_date: date (nullable = true)\n",
      "\n",
      "Count of dataframe: 1000\n"
     ]
    }
   ],
   "source": [
    "df_orders.printSchema()\n",
    "print(\"Count of dataframe:\",df_orders.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77277f9d-221c-4b07-a8f9-c475d90489c7",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b7c87-e6f2-4f77-b97a-627e065d881d",
   "metadata": {},
   "source": [
    "#### Cleanning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1587f0-4d9c-40d8-a6b9-e19d4e6147a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of after cleanning dataframe: 1000\n",
      "+--------+-----------+-------+-------------------+-------------+\n",
      "|order_id|customer_id|payment|         order_date|delivery_date|\n",
      "+--------+-----------+-------+-------------------+-------------+\n",
      "|      55|        873|  55650|2021-04-08 00:00:00|   2021-04-25|\n",
      "|      66|        631|  42774|2021-08-14 00:00:00|   2021-08-22|\n",
      "|     104|        366|  11454|2021-06-05 00:00:00|   2021-06-23|\n",
      "|     148|        573|  14073|2021-01-14 00:00:00|   2021-01-24|\n",
      "|     157|        909|  17955|2021-04-07 00:00:00|   2021-04-17|\n",
      "|     513|        950|  49647|2021-05-24 00:00:00|   2021-06-04|\n",
      "|     811|        764|  23731|2021-10-22 00:00:00|   2021-10-24|\n",
      "|     834|        139|  27691|2021-02-23 00:00:00|   2021-03-08|\n",
      "|     984|        359|  10390|2021-09-23 00:00:00|   2021-09-30|\n",
      "|      99|        291|  26787|2021-09-13 00:00:00|   2021-09-25|\n",
      "|     309|        390|  26229|2021-01-07 00:00:00|   2021-01-25|\n",
      "|     633|        183|  30197|2021-07-02 00:00:00|   2021-07-13|\n",
      "|     640|        873|  11002|2021-01-23 00:00:00|   2021-01-26|\n",
      "|     688|        574|  55980|2021-03-08 00:00:00|   2021-03-29|\n",
      "|     721|        755|  59414|2021-08-28 00:00:00|   2021-09-04|\n",
      "|     945|         54|  11172|2021-07-04 00:00:00|   2021-07-22|\n",
      "|     187|        401|  15167|2021-01-10 00:00:00|   2021-01-19|\n",
      "|     238|        328|  59828|2021-03-17 00:00:00|   2021-03-27|\n",
      "|     894|       1000|  35622|2021-10-11 00:00:00|   2021-10-13|\n",
      "|     899|        408|  32250|2021-08-04 00:00:00|   2021-08-14|\n",
      "+--------+-----------+-------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Handling Missing Values\n",
    "# Drop rows with any null values\n",
    "df_orders= df_orders.dropna()\n",
    "\n",
    "# Handling Duplicates\n",
    "df_orders =df_orders.dropDuplicates()\n",
    "print(\"Count of after cleanning dataframe:\",df_orders.count())\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ed9728-51c3-495e-b6c1-f35cc16a3050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- delivery_date: date (nullable = true)\n",
      "\n",
      "+--------+-----------+-------+----------+-------------+\n",
      "|order_id|customer_id|payment|order_date|delivery_date|\n",
      "+--------+-----------+-------+----------+-------------+\n",
      "|      55|        873|  55650|2021-04-08|   2021-04-25|\n",
      "|      66|        631|  42774|2021-08-14|   2021-08-22|\n",
      "|     104|        366|  11454|2021-06-05|   2021-06-23|\n",
      "|     148|        573|  14073|2021-01-14|   2021-01-24|\n",
      "|     157|        909|  17955|2021-04-07|   2021-04-17|\n",
      "|     513|        950|  49647|2021-05-24|   2021-06-04|\n",
      "|     811|        764|  23731|2021-10-22|   2021-10-24|\n",
      "|     834|        139|  27691|2021-02-23|   2021-03-08|\n",
      "|     984|        359|  10390|2021-09-23|   2021-09-30|\n",
      "|      99|        291|  26787|2021-09-13|   2021-09-25|\n",
      "|     309|        390|  26229|2021-01-07|   2021-01-25|\n",
      "|     633|        183|  30197|2021-07-02|   2021-07-13|\n",
      "|     640|        873|  11002|2021-01-23|   2021-01-26|\n",
      "|     688|        574|  55980|2021-03-08|   2021-03-29|\n",
      "|     721|        755|  59414|2021-08-28|   2021-09-04|\n",
      "|     945|         54|  11172|2021-07-04|   2021-07-22|\n",
      "|     187|        401|  15167|2021-01-10|   2021-01-19|\n",
      "|     238|        328|  59828|2021-03-17|   2021-03-27|\n",
      "|     894|       1000|  35622|2021-10-11|   2021-10-13|\n",
      "|     899|        408|  32250|2021-08-04|   2021-08-14|\n",
      "+--------+-----------+-------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Assuming 'order_date' is your timestamp column\n",
    "df_orders= df_orders.withColumn('order_date', to_date(col('order_date')))\n",
    "\n",
    "# Show the updated schema to verify the data type change\n",
    "df_orders.printSchema()\n",
    "df_orders. show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d459d2e-9d77-4d7d-baeb-cb59e0f6a78c",
   "metadata": {},
   "source": [
    "**What are the counts of unique orders and unique customers in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9de9714-ca0c-4474-9193-1ee03770365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n",
      "|unique_orders_count|unique_customers_count|\n",
      "+-------------------+----------------------+\n",
      "|               1000|                   617|\n",
      "+-------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregations for order_df DataFrame\n",
    "order_aggregations = df_orders.agg(\n",
    "    F.countDistinct(\"order_id\").alias(\"unique_orders_count\"),\n",
    "    F.countDistinct(\"customer_id\").alias(\"unique_customers_count\"),  \n",
    ")\n",
    "# Show the aggregated results\n",
    "order_aggregations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea79314-6036-4910-88ef-3497972e138b",
   "metadata": {},
   "source": [
    "**What is the order frequency for each customer in the datas?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80807456-2de9-4a37-83b0-f5eb361fa160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|customer_id|order_frequency|\n",
      "+-----------+---------------+\n",
      "|        833|              2|\n",
      "|        148|              1|\n",
      "|        463|              2|\n",
      "|        471|              1|\n",
      "|        496|              1|\n",
      "|        737|              2|\n",
      "|        623|              3|\n",
      "|        516|              1|\n",
      "|         85|              1|\n",
      "|        251|              1|\n",
      "|        808|              1|\n",
      "|        883|              1|\n",
      "|        458|              1|\n",
      "|        898|              2|\n",
      "|        588|              2|\n",
      "|        799|              2|\n",
      "|        970|              1|\n",
      "|        472|              2|\n",
      "|        133|              1|\n",
      "|        853|              1|\n",
      "|         78|              2|\n",
      "|        918|              1|\n",
      "|        513|              1|\n",
      "|        974|              1|\n",
      "|        321|              1|\n",
      "|        857|              1|\n",
      "|        362|              1|\n",
      "|        673|              1|\n",
      "|        375|              2|\n",
      "|        876|              1|\n",
      "+-----------+---------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "# Group by 'customer_id' and aggregate with count on 'order_id' to get order frequency\n",
    "df_order_frequency = df_orders.groupBy('customer_id').agg(count('order_id').alias('order_frequency'))\n",
    "# Show the results\n",
    "df_order_frequency.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec878a7-7585-4d8f-b7dc-52bd74fd5235",
   "metadata": {},
   "source": [
    "\r\n",
    "**How many orders were placed on each day in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e462a784-11e8-4583-9d1d-a18b8c69e382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|order_date|count|\n",
      "+----------+-----+\n",
      "|2021-01-01|    3|\n",
      "|2021-01-02|    7|\n",
      "|2021-01-03|    3|\n",
      "|2021-01-04|    3|\n",
      "|2021-01-05|    5|\n",
      "|2021-01-06|    2|\n",
      "|2021-01-07|    5|\n",
      "|2021-01-08|    2|\n",
      "|2021-01-09|    5|\n",
      "|2021-01-10|    6|\n",
      "|2021-01-11|    4|\n",
      "|2021-01-12|    7|\n",
      "|2021-01-13|    4|\n",
      "|2021-01-14|    3|\n",
      "|2021-01-15|    3|\n",
      "|2021-01-16|    1|\n",
      "|2021-01-17|    2|\n",
      "|2021-01-18|    2|\n",
      "|2021-01-19|    1|\n",
      "|2021-01-20|    4|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_per_day = df_orders.groupBy(\"order_date\").count().orderBy(\"order_date\")\n",
    "orders_per_day.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0d1b2-cf3e-418e-8cc7-7bfa84dbb1b8",
   "metadata": {},
   "source": [
    "* show sales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb05e09f-3c95-4323-91f7-f7c410a9eeb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "|sales_id|order_id|product_id|price_per_unit|quantity|total_price|\n",
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "|       0|       1|       218|           106|       2|        212|\n",
      "|       1|       1|       481|           118|       1|        118|\n",
      "|       2|       1|         2|            96|       3|        288|\n",
      "|       3|       1|      1002|           106|       2|        212|\n",
      "|       4|       1|       691|           113|       3|        339|\n",
      "|       5|       1|       981|           106|       3|        318|\n",
      "|       6|       2|       915|            96|       1|         96|\n",
      "|       7|       2|       686|           113|       1|        113|\n",
      "|       8|       2|      1091|           115|       3|        345|\n",
      "|       9|       2|      1196|           105|       1|        105|\n",
      "|      10|       2|       157|            91|       3|        273|\n",
      "|      11|       2|      1174|            99|       2|        198|\n",
      "|      12|       3|        54|           104|       1|        104|\n",
      "|      13|       3|         6|            96|       2|        192|\n",
      "|      14|       3|      1010|           106|       2|        212|\n",
      "|      15|       4|      1033|            90|       1|         90|\n",
      "|      16|       4|      1184|            99|       3|        297|\n",
      "|      17|       4|       769|           109|       1|        109|\n",
      "|      18|       4|       923|            96|       3|        288|\n",
      "|      19|       4|        12|            96|       2|        192|\n",
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\sales.csv\"\n",
    "df_sales = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6609a514-8cf1-436b-a41b-fd3b2f017ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price_per_unit: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- total_price: integer (nullable = true)\n",
      "\n",
      "Count of dataframe: 5000\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()\n",
    "print(\"Count of dataframe:\",df_sales.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25a92132-6899-406c-845e-706e18652422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+\n",
      "|summary|   price_per_unit|          quantity|      total_price|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "|  count|             5000|              5000|             5000|\n",
      "|   mean|         103.5016|            1.9924|           206.36|\n",
      "| stddev|9.195004462283432|0.8075101575403906|86.35745666741475|\n",
      "|    min|               90|                 1|               90|\n",
      "|    max|              119|                 3|              357|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.describe([\"price_per_unit\", \"quantity\", \"total_price\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee58bbc9-5e81-4e09-8e58-48be523208d9",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e43d55-a529-46f4-a5a1-acc1324e9527",
   "metadata": {},
   "source": [
    "#### Cleanning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9a5d780-c8ab-4ec2-b93e-c62b3d5f9245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of after cleanning dataframe: 5000\n",
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "|sales_id|order_id|product_id|price_per_unit|quantity|total_price|\n",
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "|     797|     166|       217|           106|       2|        212|\n",
      "|     970|     199|      1057|            93|       3|        279|\n",
      "|    1129|     233|       218|           106|       3|        318|\n",
      "|    1841|     371|      1013|           106|       1|        106|\n",
      "|    2058|     415|       487|           118|       2|        236|\n",
      "|    2085|     419|       581|           100|       2|        200|\n",
      "|    2145|     432|       191|            98|       3|        294|\n",
      "|    2474|     501|      1029|            90|       2|        180|\n",
      "|    2488|     503|       974|            97|       1|         97|\n",
      "|    2584|     519|       161|            91|       2|        182|\n",
      "|    2610|     523|       727|           119|       3|        357|\n",
      "|    2640|     528|       472|           118|       3|        354|\n",
      "|    3176|     640|      1011|           106|       1|        106|\n",
      "|    3518|     707|       234|           106|       2|        212|\n",
      "|    4378|     874|      1170|            99|       1|         99|\n",
      "|       3|       1|      1002|           106|       2|        212|\n",
      "|      35|       8|       256|            95|       2|        190|\n",
      "|     573|     123|       700|           119|       3|        357|\n",
      "|     846|     175|        66|           104|       1|        104|\n",
      "|    1042|     213|        55|           104|       1|        104|\n",
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Handling Missing Values\n",
    "# Drop rows with any null values\n",
    "df_sales=df_sales.dropna()\n",
    "\n",
    "# Handling Duplicates\n",
    "df_saless =df_sales.dropDuplicates()\n",
    "print(\"Count of after cleanning dataframe:\",df_sales.count())\n",
    "df_saless.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937fd614-49c7-4d3f-963e-9be2757266e9",
   "metadata": {},
   "source": [
    "\n",
    "**What is the average price per unit, total revenue, and total quantity sold?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6e14d18-c6eb-4214-bd5f-5bde7ba338a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+---------------------+------------------+-------------+-------------------+\n",
      "|unique_sales_count|unique_orders_count|unique_products_count|avg_price_per_unit|total_revenue|total_quantity_sold|\n",
      "+------------------+-------------------+---------------------+------------------+-------------+-------------------+\n",
      "|              5000|                993|                 1233|          103.5016|   5155414696|               9962|\n",
      "+------------------+-------------------+---------------------+------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregations for df_sales DataFrame\n",
    "sales_aggregations = df_sales.agg(\n",
    "    F.countDistinct(\"sales_id\").alias(\"unique_sales_count\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"unique_orders_count\"),\n",
    "    F.countDistinct(\"product_id\").alias(\"unique_products_count\"),\n",
    "    F.avg(\"price_per_unit\").alias(\"avg_price_per_unit\"),\n",
    "    (F.sum(\"price_per_unit\") * F.sum(\"quantity\")).alias(\"total_revenue\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity_sold\")\n",
    ")\n",
    "\n",
    "# Show the aggregated results\n",
    "sales_aggregations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79818a0-ef06-40f5-9b51-89d9156e0e6a",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "**What is the total sales amount for each product, and which products have the highest total sales?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2da66e5a-8fdb-4519-aba6-fd6779fbdcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|product_id|sum(total_price)|\n",
      "+----------+----------------+\n",
      "|        78|            2832|\n",
      "|       472|            2714|\n",
      "|       707|            2499|\n",
      "|       579|            2400|\n",
      "|       843|            2373|\n",
      "|       486|            2360|\n",
      "|       740|            2289|\n",
      "|       727|            2261|\n",
      "|       182|            2254|\n",
      "|       465|            2124|\n",
      "|        95|            2124|\n",
      "|        74|            2124|\n",
      "|       810|            2106|\n",
      "|       316|            2071|\n",
      "|       405|            2023|\n",
      "|       222|            2014|\n",
      "|       830|            1989|\n",
      "|      1188|            1980|\n",
      "|      1184|            1980|\n",
      "|      1091|            1955|\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_by_product = df_sales.groupBy('product_id').sum('total_price').orderBy('sum(total_price)', ascending=False)\n",
    "sales_by_product.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b134a4b-36fa-4ce2-aa7c-a5371d878fac",
   "metadata": {},
   "source": [
    "* show products dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c778e4b8-9e4d-40ae-9a6c-b48c97354c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+----+------+-----+--------+--------------------+\n",
      "|product_ID|product_type|product_name|size|colour|price|quantity|         description|\n",
      "+----------+------------+------------+----+------+-----+--------+--------------------+\n",
      "|         0|       Shirt|Oxford Cloth|  XS|   red|  114|      66|A red coloured, X...|\n",
      "|         1|       Shirt|Oxford Cloth|   S|   red|  114|      53|A red coloured, S...|\n",
      "|         2|       Shirt|Oxford Cloth|   M|   red|  114|      54|A red coloured, M...|\n",
      "|         3|       Shirt|Oxford Cloth|   L|   red|  114|      69|A red coloured, L...|\n",
      "|         4|       Shirt|Oxford Cloth|  XL|   red|  114|      47|A red coloured, X...|\n",
      "|         5|       Shirt|Oxford Cloth|  XS|orange|  114|      45|A orange coloured...|\n",
      "|         6|       Shirt|Oxford Cloth|   S|orange|  114|      72|A orange coloured...|\n",
      "|         7|       Shirt|Oxford Cloth|   M|orange|  114|      77|A orange coloured...|\n",
      "|         8|       Shirt|Oxford Cloth|   L|orange|  114|      48|A orange coloured...|\n",
      "|         9|       Shirt|Oxford Cloth|  XL|orange|  114|      43|A orange coloured...|\n",
      "|        10|       Shirt|Oxford Cloth|  XS|yellow|  114|      72|A yellow coloured...|\n",
      "|        11|       Shirt|Oxford Cloth|   S|yellow|  114|      78|A yellow coloured...|\n",
      "|        12|       Shirt|Oxford Cloth|   M|yellow|  114|      56|A yellow coloured...|\n",
      "|        13|       Shirt|Oxford Cloth|   L|yellow|  114|      75|A yellow coloured...|\n",
      "|        14|       Shirt|Oxford Cloth|  XL|yellow|  114|      50|A yellow coloured...|\n",
      "|        15|       Shirt|Oxford Cloth|  XS| green|  114|      68|A green coloured,...|\n",
      "|        16|       Shirt|Oxford Cloth|   S| green|  114|      56|A green coloured,...|\n",
      "|        17|       Shirt|Oxford Cloth|   M| green|  114|      60|A green coloured,...|\n",
      "|        18|       Shirt|Oxford Cloth|   L| green|  114|      52|A green coloured,...|\n",
      "|        19|       Shirt|Oxford Cloth|  XL| green|  114|      42|A green coloured,...|\n",
      "+----------+------------+------------+----+------+-----+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\products.csv\"\n",
    "df_products = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "# Show the first 10 rows of the DataFrame\n",
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e08aba01-bb84-4870-9920-fac27d44847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_ID: integer (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- colour: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 1260\n"
     ]
    }
   ],
   "source": [
    "df_products.printSchema()\n",
    "print(\"Count of dataframe:\",df_products.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d8138-8ddd-4b35-a81d-224209ec602c",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d5910-fce6-4dec-b14b-e41eaff0a579",
   "metadata": {},
   "source": [
    "#### Cleanning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b562112-e9e2-4f59-8d1f-7f02678995dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of after cleanning dataframe: 1260\n",
      "+----------+------------+---------------+----+------+-----+--------+--------------------+\n",
      "|product_ID|product_type|   product_name|size|colour|price|quantity|         description|\n",
      "+----------+------------+---------------+----+------+-----+--------+--------------------+\n",
      "|       266|       Shirt|           Polo|   S|  blue|  117|      76|A blue coloured, ...|\n",
      "|       299|       Shirt|   Cuban Collar|  XL| green|   93|      60|A green coloured,...|\n",
      "|       439|      Jacket|          Denim|  XL| green|   92|      59|A green coloured,...|\n",
      "|       458|      Jacket|         Puffer|   L|   red|  110|      44|A red coloured, L...|\n",
      "|       856|    Trousers|         Chinos|   S| green|  100|      61|A green coloured,...|\n",
      "|      1032|    Trousers|    Relaxed Leg|   M| green|   95|      43|A green coloured,...|\n",
      "|      1040|    Trousers|    Relaxed Leg|  XS|indigo|   95|      60|A indigo coloured...|\n",
      "|      1160|    Trousers|   High-Waisted|  XS|orange|   98|      55|A orange coloured...|\n",
      "|       338|       Shirt|Mandarin Collar|   L|  blue|  108|      68|A blue coloured, ...|\n",
      "|       972|    Trousers|       Slim-Fit|   M|indigo|  119|      67|A indigo coloured...|\n",
      "|       986|    Trousers|           Wool|   S|orange|  111|      59|A orange coloured...|\n",
      "|      1066|    Trousers|        Joggers|   S| green|   94|      76|A green coloured,...|\n",
      "|        79|       Shirt|        Flannel|  XL|orange|   96|      50|A orange coloured...|\n",
      "|       477|      Jacket|         Puffer|   M|  blue|  110|      60|A blue coloured, ...|\n",
      "|       483|      Jacket|         Puffer|   L|indigo|  110|      61|A indigo coloured...|\n",
      "|       486|      Jacket|         Puffer|   S|violet|  110|      46|A violet coloured...|\n",
      "|       565|      Jacket|         Bomber|  XS|orange|   90|      80|A orange coloured...|\n",
      "|       749|      Jacket|        Peacoat|  XL|yellow|  102|      53|A yellow coloured...|\n",
      "|       901|    Trousers|          Cords|   S|indigo|  113|      60|A indigo coloured...|\n",
      "|      1170|    Trousers|   High-Waisted|  XS| green|   98|      42|A green coloured,...|\n",
      "+----------+------------+---------------+----+------+-----+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Values\n",
    "# Drop rows with any null values\n",
    "df_products= df_products.dropna()\n",
    "\n",
    "# Handling Duplicates\n",
    "df_products =df_products.dropDuplicates()\n",
    "print(\"Count of after cleanning dataframe:\",df_products.count())\n",
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dc82940-37e2-472a-a981-4536a821c23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------------------+--------------+\n",
      "|unique_products_count|total_products|     average_price|total_quantity|\n",
      "+---------------------+--------------+------------------+--------------+\n",
      "|                 1260|          1260|105.80555555555556|         75789|\n",
      "+---------------------+--------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregations for product DataFrame\n",
    "product_aggregations = df_products.agg(\n",
    "    F.countDistinct(\"product_ID\").alias(\"unique_products_count\"),\n",
    "    F.count(\"product_type\").alias(\"total_products\"),\n",
    "    F.avg(\"price\").alias(\"average_price\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity\")\n",
    ")\n",
    "\n",
    "# Show the aggregated results\n",
    "product_aggregations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eb38088-3b58-497a-a7b5-d966430058ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|product_type|count|\n",
      "+------------+-----+\n",
      "|      Jacket|  420|\n",
      "|    Trousers|  420|\n",
      "|       Shirt|  420|\n",
      "+------------+-----+\n",
      "\n",
      "+------------+------------------+\n",
      "|product_type|         avg_price|\n",
      "+------------+------------------+\n",
      "|      Jacket|107.41666666666667|\n",
      "|    Trousers|101.66666666666667|\n",
      "|       Shirt|108.33333333333333|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of Products by Type\n",
    "product_type_counts = df_products.groupBy(\"product_type\").count().orderBy(\"count\", ascending=False)\n",
    "# Show product type counts\n",
    "product_type_counts.show()\n",
    "# Statistical Analysis by Product Type (example: average price)\n",
    "avg_price_by_type = df_products.groupBy(\"product_type\").agg(F.avg(\"price\").alias(\"avg_price\"))\n",
    "# Show average price by product type\n",
    "avg_price_by_type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d2885-832b-4ee8-91b2-fff6eeb51b06",
   "metadata": {},
   "source": [
    "\r\n",
    "**What is the total price for each produ a and how does it vary across different products?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f77ad5c-3d8d-45d8-ab3d-df2ff21ba042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------------+----+------+-----+--------+--------------------------------------------------+-----------+\n",
      "|product_ID|product_type|product_name   |size|colour|price|quantity|description                                       |total_price|\n",
      "+----------+------------+---------------+----+------+-----+--------+--------------------------------------------------+-----------+\n",
      "|266       |Shirt       |Polo           |S   |blue  |117  |76      |A blue coloured, S sized, Polo Shirt              |8892       |\n",
      "|299       |Shirt       |Cuban Collar   |XL  |green |93   |60      |A green coloured, XL sized, Cuban Collar Shirt    |5580       |\n",
      "|439       |Jacket      |Denim          |XL  |green |92   |59      |A green coloured, XL sized, Denim Jacket          |5428       |\n",
      "|458       |Jacket      |Puffer         |L   |red   |110  |44      |A red coloured, L sized, Puffer Jacket            |4840       |\n",
      "|856       |Trousers    |Chinos         |S   |green |100  |61      |A green coloured, S sized, Chinos Trousers        |6100       |\n",
      "|1032      |Trousers    |Relaxed Leg    |M   |green |95   |43      |A green coloured, M sized, Relaxed Leg Trousers   |4085       |\n",
      "|1040      |Trousers    |Relaxed Leg    |XS  |indigo|95   |60      |A indigo coloured, XS sized, Relaxed Leg Trousers |5700       |\n",
      "|1160      |Trousers    |High-Waisted   |XS  |orange|98   |55      |A orange coloured, XS sized, High-Waisted Trousers|5390       |\n",
      "|338       |Shirt       |Mandarin Collar|L   |blue  |108  |68      |A blue coloured, L sized, Mandarin Collar Shirt   |7344       |\n",
      "|972       |Trousers    |Slim-Fit       |M   |indigo|119  |67      |A indigo coloured, M sized, Slim-Fit Trousers     |7973       |\n",
      "|986       |Trousers    |Wool           |S   |orange|111  |59      |A orange coloured, S sized, Wool Trousers         |6549       |\n",
      "|1066      |Trousers    |Joggers        |S   |green |94   |76      |A green coloured, S sized, Joggers Trousers       |7144       |\n",
      "|79        |Shirt       |Flannel        |XL  |orange|96   |50      |A orange coloured, XL sized, Flannel Shirt        |4800       |\n",
      "|477       |Jacket      |Puffer         |M   |blue  |110  |60      |A blue coloured, M sized, Puffer Jacket           |6600       |\n",
      "|483       |Jacket      |Puffer         |L   |indigo|110  |61      |A indigo coloured, L sized, Puffer Jacket         |6710       |\n",
      "|486       |Jacket      |Puffer         |S   |violet|110  |46      |A violet coloured, S sized, Puffer Jacket         |5060       |\n",
      "|565       |Jacket      |Bomber         |XS  |orange|90   |80      |A orange coloured, XS sized, Bomber Jacket        |7200       |\n",
      "|749       |Jacket      |Peacoat        |XL  |yellow|102  |53      |A yellow coloured, XL sized, Peacoat Jacket       |5406       |\n",
      "|901       |Trousers    |Cords          |S   |indigo|113  |60      |A indigo coloured, S sized, Cords Trousers        |6780       |\n",
      "|1170      |Trousers    |High-Waisted   |XS  |green |98   |42      |A green coloured, XS sized, High-Waisted Trousers |4116       |\n",
      "+----------+------------+---------------+----+------+-----+--------+--------------------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Calculate total price for each product\n",
    "df_total_price = df_products.withColumn(\"total_price\", col(\"quantity\") * col(\"price\"))\n",
    "\n",
    "# Show the DataFrame with total price\n",
    "df_total_price.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e0d66-2463-4c7d-89a9-8f03a8411e69",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**What is the total revenue generated for each product type and color combination in the dataset?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04e7c708-376b-43f3-9c27-1e571e9f7676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----------+\n",
      "|product_type|colour|total_price|\n",
      "+------------+------+-----------+\n",
      "|Shirt       |red   |389840     |\n",
      "|Trousers    |orange|375139     |\n",
      "|Jacket      |orange|403578     |\n",
      "|Trousers    |violet|368337     |\n",
      "|Jacket      |green |389454     |\n",
      "|Trousers    |blue  |353392     |\n",
      "|Jacket      |red   |386938     |\n",
      "|Jacket      |violet|403096     |\n",
      "|Trousers    |yellow|367561     |\n",
      "|Shirt       |yellow|399579     |\n",
      "|Jacket      |yellow|377513     |\n",
      "|Shirt       |blue  |393531     |\n",
      "|Shirt       |indigo|392906     |\n",
      "|Jacket      |blue  |387448     |\n",
      "|Jacket      |indigo|381438     |\n",
      "|Shirt       |orange|382718     |\n",
      "|Trousers    |indigo|348804     |\n",
      "|Trousers    |red   |381457     |\n",
      "|Trousers    |green |364529     |\n",
      "|Shirt       |violet|393587     |\n",
      "+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total price for each product\n",
    "df_total_price_by_type_color = df_products.withColumn(\"total_price\", col(\"quantity\") * col(\"price\")) \\\n",
    "    .groupBy(\"product_type\", \"colour\") \\\n",
    "    .agg(sum(\"total_price\").alias(\"total_price\"))\n",
    "\n",
    "# Show the DataFrame with total price by product type and color\n",
    "df_total_price_by_type_color.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80b7cef1-bab0-40c7-b9b5-d8d61a349719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----------+----+\n",
      "|product_type|colour|total_price|rank|\n",
      "+------------+------+-----------+----+\n",
      "|Jacket      |orange|403578     |1   |\n",
      "|Jacket      |violet|403096     |2   |\n",
      "|Jacket      |green |389454     |3   |\n",
      "|Jacket      |blue  |387448     |4   |\n",
      "|Jacket      |red   |386938     |5   |\n",
      "|Jacket      |indigo|381438     |6   |\n",
      "|Jacket      |yellow|377513     |7   |\n",
      "|Shirt       |yellow|399579     |1   |\n",
      "|Shirt       |violet|393587     |2   |\n",
      "|Shirt       |blue  |393531     |3   |\n",
      "|Shirt       |indigo|392906     |4   |\n",
      "|Shirt       |red   |389840     |5   |\n",
      "|Shirt       |green |387925     |6   |\n",
      "|Shirt       |orange|382718     |7   |\n",
      "|Trousers    |red   |381457     |1   |\n",
      "|Trousers    |orange|375139     |2   |\n",
      "|Trousers    |violet|368337     |3   |\n",
      "|Trousers    |yellow|367561     |4   |\n",
      "|Trousers    |green |364529     |5   |\n",
      "|Trousers    |blue  |353392     |6   |\n",
      "+------------+------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum, rank\n",
    "\n",
    "# Define a Window partitioned by product_type and ordered by total_price descending\n",
    "windowSpec = Window.partitionBy(\"product_type\").orderBy(col(\"total_price\").desc())\n",
    "\n",
    "# Add a rank column based on total_price within each product_type\n",
    "df_ranked = df_total_price_by_type_color.withColumn(\"rank\", rank().over(windowSpec))\n",
    "\n",
    "# Show the DataFrame with total price, product type, color, and rank\n",
    "df_ranked.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c9cdd-0809-4d54-8790-41560cd8ce49",
   "metadata": {},
   "source": [
    "#### Perform the join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feeb36f4-bc6e-4048-83e0-e9654fc8cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------+\n",
      "|order_id|total_amount_per_order|\n",
      "+--------+----------------------+\n",
      "|1       |1487                  |\n",
      "|2       |1130                  |\n",
      "|3       |508                   |\n",
      "|4       |976                   |\n",
      "|5       |2043                  |\n",
      "|6       |732                   |\n",
      "|7       |523                   |\n",
      "|8       |299                   |\n",
      "|9       |1315                  |\n",
      "|10      |874                   |\n",
      "|11      |461                   |\n",
      "|12      |714                   |\n",
      "|13      |1583                  |\n",
      "|14      |1254                  |\n",
      "|15      |1365                  |\n",
      "|16      |1786                  |\n",
      "|17      |670                   |\n",
      "|18      |952                   |\n",
      "|19      |1244                  |\n",
      "|20      |1557                  |\n",
      "+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_sales is your DataFrame with sales data\n",
    "# Calculate the total amount spent per order ID\n",
    "df_total_amount = df_sales.withColumn(\"total_amount\", col(\"price_per_unit\") * col(\"quantity\")) \\\n",
    "    .groupBy(\"order_id\") \\\n",
    "    .agg(sum(\"total_amount\").alias(\"total_amount_per_order\")) \\\n",
    "    .orderBy(\"order_id\")\n",
    "\n",
    "# Show the total amount spent per order ID\n",
    "df_total_amount.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcc6c618-d271-4321-86d7-bec195d08b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+----------+-------------+----------------------+\n",
      "|order_id|customer_id|payment|order_date|delivery_date|total_amount_per_order|\n",
      "+--------+-----------+-------+----------+-------------+----------------------+\n",
      "|77      |680        |56675  |2021-04-19|2021-04-29   |2733                  |\n",
      "|252     |683        |11465  |2021-01-11|2021-02-05   |2597                  |\n",
      "|966     |639        |28100  |2021-02-25|2021-03-10   |2550                  |\n",
      "|668     |22         |40483  |2021-03-15|2021-03-30   |2539                  |\n",
      "|705     |107        |11686  |2021-01-14|2021-01-25   |2495                  |\n",
      "|142     |237        |53165  |2021-06-15|2021-07-12   |2469                  |\n",
      "|734     |623        |30153  |2021-01-25|2021-01-31   |2455                  |\n",
      "|530     |467        |20275  |2021-01-07|2021-01-23   |2450                  |\n",
      "|970     |382        |36941  |2021-01-04|2021-01-23   |2390                  |\n",
      "|936     |148        |47197  |2021-07-16|2021-08-01   |2376                  |\n",
      "|427     |282        |20082  |2021-01-27|2021-02-10   |2333                  |\n",
      "|113     |565        |49592  |2021-03-19|2021-04-08   |2316                  |\n",
      "|545     |439        |53117  |2021-06-21|2021-07-02   |2300                  |\n",
      "|517     |797        |37440  |2021-06-23|2021-06-30   |2298                  |\n",
      "|940     |813        |11980  |2021-09-11|2021-10-04   |2289                  |\n",
      "|312     |683        |38591  |2021-07-06|2021-07-31   |2279                  |\n",
      "|494     |740        |37675  |2021-07-31|2021-08-15   |2268                  |\n",
      "|341     |602        |13882  |2021-02-07|2021-02-09   |2267                  |\n",
      "|28      |206        |29968  |2021-08-19|2021-09-04   |2267                  |\n",
      "|890     |649        |54889  |2021-06-21|2021-07-07   |2264                  |\n",
      "+--------+-----------+-------+----------+-------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform the join operation\n",
    "df_merged = df_orders.join(df_total_amount, on='order_id', how='left')\n",
    "\n",
    "# Sort the merged DataFrame by the 'total_amount' column in descending order\n",
    "df_merged_sorted = df_merged.orderBy('total_amount_per_order', ascending=False)\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "df_merged_sorted.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d4aebb0-a2ac-4d4d-b303-ca961831475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+----------+-------------+----------------------+--------------------+------+---+-----------------------------+--------+------------------+----------------------------+\n",
      "|customer_id|order_id|payment|order_date|delivery_date|total_amount_per_order|customer_name       |gender|age|home_address                 |zip_code|city              |state                       |\n",
      "+-----------+--------+-------+----------+-------------+----------------------+--------------------+------+---+-----------------------------+--------+------------------+----------------------------+\n",
      "|873        |55      |55650  |2021-04-08|2021-04-25   |119                   |Cecilla Pigford     |Other |38 |5538 Jett MallSuite 854      |5510    |Samanthatown      |Western Australia           |\n",
      "|631        |66      |42774  |2021-08-14|2021-08-22   |2077                  |Wanids Cudby        |Female|49 |80 Chelsea KnollApt. 014     |7749    |Davidfurt         |Northern Territory          |\n",
      "|366        |104     |11454  |2021-06-05|2021-06-23   |1141                  |Geraldine Naulty    |Other |64 |491 Fisher IslandSuite 745   |1110    |Youngview         |Victoria                    |\n",
      "|573        |148     |14073  |2021-01-14|2021-01-24   |1012                  |Atlante Dissman     |Male  |37 |80 Charles CircuitSuite 023  |7541    |Abbeyshire        |South Australia             |\n",
      "|909        |157     |17955  |2021-04-07|2021-04-17   |699                   |Patin Circuit       |Other |62 |976 Murray Station StApt. 036|3227    |Langfort          |Tasmania                    |\n",
      "|950        |513     |49647  |2021-05-24|2021-06-04   |319                   |Eustacia Andreassen |Male  |43 |3414 Mia MallApt. 446        |3051    |Bergefort         |Victoria                    |\n",
      "|764        |811     |23731  |2021-10-22|2021-10-24   |616                   |Brod Phifer         |Other |32 |05 Fahey CircleSuite 721     |9998    |Hettingerfort     |Western Australia           |\n",
      "|139        |834     |27691  |2021-02-23|2021-03-08   |923                   |Vida Matchitt       |Male  |73 |69 Aiden ParkwaySuite 721    |8569    |Hudsonberg        |Australian Capital Territory|\n",
      "|359        |984     |10390  |2021-09-23|2021-09-30   |2074                  |Augustin Brandenberg|Other |28 |20 Skye RunApt. 868          |5653    |East Lauren       |Victoria                    |\n",
      "|291        |99      |26787  |2021-09-13|2021-09-25   |1017                  |Brodie Hedin        |Other |43 |6428 Boyle MallApt. 711      |2048    |South Hannah      |Tasmania                    |\n",
      "|390        |309     |26229  |2021-01-07|2021-01-25   |307                   |Bee Mazonowicz      |Other |36 |806 O'connell CrestApt. 665  |7155    |South Eveshire    |Western Australia           |\n",
      "|183        |633     |30197  |2021-07-02|2021-07-13   |1431                  |Martin O'Mailey     |Female|44 |639 Griffiths ParkwayApt. 162|9583    |New Angelina      |Queensland                  |\n",
      "|873        |640     |11002  |2021-01-23|2021-01-26   |106                   |Cecilla Pigford     |Other |38 |5538 Jett MallSuite 854      |5510    |Samanthatown      |Western Australia           |\n",
      "|574        |688     |55980  |2021-03-08|2021-03-29   |963                   |Ranna Paolillo      |Other |53 |1804 Ella HillSuite 318      |9400    |Zacharybury       |Northern Territory          |\n",
      "|755        |721     |59414  |2021-08-28|2021-09-04   |1178                  |Herbert Rock        |Female|79 |04 Dare TerraceSuite 650     |9976    |West Aaron        |New South Wales             |\n",
      "|54         |945     |11172  |2021-07-04|2021-07-22   |1058                  |Tore Caras          |Other |62 |2912 Adams JunctionSuite 740 |5575    |West Alyssa       |Queensland                  |\n",
      "|401        |187     |15167  |2021-01-10|2021-01-19   |1195                  |Waylen Swallwell    |Other |73 |254 Layla AvenueSuite 506    |613     |Sarahstad         |Western Australia           |\n",
      "|328        |238     |59828  |2021-03-17|2021-03-27   |1432                  |Wadsworth Olivier   |Other |59 |4309 Nguyen KnollApt. 851    |6795    |North Elijahshire |Western Australia           |\n",
      "|1000       |894     |35622  |2021-10-11|2021-10-13   |1483                  |Mandel Fairbanks    |Male  |71 |1671 Lauren KnollSuite 945   |9012    |Lake Audreyborough|Tasmania                    |\n",
      "|408        |899     |32250  |2021-08-04|2021-08-14   |1085                  |Alis MacCulloch     |Other |66 |8142 Jackson ParkwaySuite 475|5079    |Lake Haydenton    |Australian Capital Territory|\n",
      "+-----------+--------+-------+----------+-------------+----------------------+--------------------+------+---+-----------------------------+--------+------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the join operation using customer_id\n",
    "df_merged2 = df_merged.join(df_customers, on='customer_id', how='left')\n",
    "\n",
    "# Show the merged DataFrame\n",
    "df_merged2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4576f668-9774-4297-bc9b-d55e8dc87603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------+----------------------+\n",
      "|customer_id|customer_name         |total_amount_per_order|\n",
      "+-----------+----------------------+----------------------+\n",
      "|680        |Camellia Heatherington|2733                  |\n",
      "|683        |Esteban Hensmans      |2597                  |\n",
      "|639        |Tobin Jeffryes        |2550                  |\n",
      "|22         |Blayne Sabatier       |2539                  |\n",
      "|107        |Sandy Yushachkov      |2495                  |\n",
      "|237        |Cassaundra Sliman     |2469                  |\n",
      "|623        |Hort Ridding          |2455                  |\n",
      "|467        |Cacilie Forrestill    |2450                  |\n",
      "|382        |Emanuele Deesly       |2390                  |\n",
      "|148        |Ikey Spohr            |2376                  |\n",
      "|282        |Wren Helgass          |2333                  |\n",
      "|565        |Hoebart Edger         |2316                  |\n",
      "|439        |Daren Airs            |2300                  |\n",
      "|797        |Rachelle Jelf         |2298                  |\n",
      "|813        |Saul Franklyn         |2289                  |\n",
      "|683        |Esteban Hensmans      |2279                  |\n",
      "|740        |Meryl Neeson          |2268                  |\n",
      "|602        |Kip Cantwell          |2267                  |\n",
      "|206        |Crin Johnikin         |2267                  |\n",
      "|649        |Willey Tolley         |2264                  |\n",
      "+-----------+----------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "# Select specific columns to show\n",
    "df_selected = df_merged2.select(\"customer_id\", \"customer_name\", \"total_amount_per_order\")\n",
    "# Sort the DataFrame by 'total_amount_per_order' column in descending order\n",
    "df_sorted = df_selected.orderBy(desc('total_amount_per_order'))\n",
    "# Show the sorted DataFrame\n",
    "df_sorted.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
