{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb9c55e-cf4e-4521-a78a-5d573e61befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Datacamp Pyspark Tutorial\")\\\n",
    ".config(\"spark.memory.offHeap. enabled\", \"true\") . config(\"spark.memory.offHeap. size\", \"10g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3046fa34-a719-41e4-91d6-37f1b886788b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+---+-------------------------------+--------+-----------------+----------------------------+---------+\n",
      "|customer_id|customer_name      |gender     |age|home_address                   |zip_code|city             |state                       |country  |\n",
      "+-----------+-------------------+-----------+---+-------------------------------+--------+-----------------+----------------------------+---------+\n",
      "|1          |Leanna Busson      |Female     |30 |8606 Victoria TerraceSuite 560 |5464    |Johnstonhaven    |Northern Territory          |Australia|\n",
      "|2          |Zabrina Harrowsmith|Genderfluid|69 |8327 Kirlin SummitApt. 461     |8223    |New Zacharyfort  |South Australia             |Australia|\n",
      "|3          |Shina Dullaghan    |Polygender |59 |269 Gemma SummitSuite 109      |5661    |Aliburgh         |Australian Capital Territory|Australia|\n",
      "|4          |Hewet McVitie      |Bigender   |67 |743 Bailey GroveSuite 141      |1729    |South Justinhaven|Queensland                  |Australia|\n",
      "|5          |Rubia Ashleigh     |Polygender |30 |48 Hyatt ManorSuite 375        |4032    |Griffithsshire   |Queensland                  |Australia|\n",
      "|6          |Cordey Tolcher     |Genderfluid|40 |7118 Mccullough SquareSuite 639|9996    |Blakehaven       |New South Wales             |Australia|\n",
      "|7          |Winslow Ewbanck    |Bigender   |76 |92 Hills Station StApt. 683    |793     |Masonfurt        |Queensland                  |Australia|\n",
      "|8          |Marlowe Wynn       |Agender    |75 |383 Muller SummitSuite 809     |7681    |Samside          |Northern Territory          |Australia|\n",
      "|9          |Brittaney Gontier  |Male       |51 |57 Greenfelder HillApt. 077    |2       |Beierport        |Northern Territory          |Australia|\n",
      "|10         |Susanetta Wilshin  |Bigender   |70 |615 Hayley KnollSuite 454      |2118    |Joelburgh        |Western Australia           |Australia|\n",
      "+-----------+-------------------+-----------+---+-------------------------------+--------+-----------------+----------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- home_address: string (nullable = true)\n",
      " |-- zip_code: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 1000\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\customers.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "df_customers = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_customers.show()\n",
    "df_customers .printSchema()\n",
    "print(\"Count of dataframe:\",df_customers.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b566fa73-ae59-4d01-b484-efc88968b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "|summary|      customer_id| customer_name|    gender|               age|        home_address|         zip_code|     city|               state|  country|\n",
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "|  count|             1000|          1000|      1000|              1000|                1000|             1000|     1000|                1000|     1000|\n",
      "|   mean|            500.5|          null|      null|             49.86|                null|         5004.872|     null|                null|     null|\n",
      "| stddev|288.8194360957494|          null|      null|17.647828360618387|                null|2884.497332027621|     null|                null|     null|\n",
      "|    min|                1|Abbot Rickaert|   Agender|                20|00 Fadel CircuitA...|                2|Aaronbury|Australian Capita...|Australia|\n",
      "|    max|             1000|   Zulema Teml|Polygender|                80|9993 Wood RidgeAp...|             9998| Zacville|   Western Australia|Australia|\n",
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d44a6415-c5d0-4df5-9de9-2d0a0e76aa22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|               age|         zip_code|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|              1000|             1000|\n",
      "|   mean|             49.86|         5004.872|\n",
      "| stddev|17.647828360618387|2884.497332027621|\n",
      "|    min|                20|                2|\n",
      "|    max|                80|             9998|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.describe([\"age\", \"zip_code\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98a74439-7d6f-44c4-ba05-7d099ccf2535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|     gender|count|\n",
      "+-----------+-----+\n",
      "|Genderqueer|  127|\n",
      "|    Agender|  114|\n",
      "|     Female|  115|\n",
      "| Polygender|  128|\n",
      "|   Bigender|  120|\n",
      "| Non-binary|  131|\n",
      "|       Male|  143|\n",
      "|Genderfluid|  122|\n",
      "+-----------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|  country|count|\n",
      "+---------+-----+\n",
      "|Australia| 1000|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.groupBy(\"gender\").count().show()\n",
    "df_customers.groupBy(\"country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2474d387-e277-4292-8a08-2705588b914f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|customer_id|      customer_name|     gender|age|        home_address|zip_code|                city|               state|  country|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|        152|    Panchito Wybern|     Female| 79|03 Glover TrackAp...|    7845|       Christianport|Australian Capita...|Australia|\n",
      "|        540|       Maude McCaig|Genderfluid| 75|297 Ferry LaneApt...|    8550|         East Amelia|     South Australia|Australia|\n",
      "|        851|      Worthy Pardoe| Non-binary| 71|71 Joseph TrailAp...|    6648|     North Emmashire|Australian Capita...|Australia|\n",
      "|        903|   Leonhard Webland|       Male| 48|9694 Nathan Trail...|    2370|          Walkerfurt|     New South Wales|Australia|\n",
      "|        941|   Egon Figliovanni|     Female| 59|617 Morrison Cres...|     105|      Farrellchester|     New South Wales|Australia|\n",
      "|        227|    Isaak Ferrarone| Non-binary| 26|074 Hackett Parad...|    3857|         South Holly|            Tasmania|Australia|\n",
      "|        484|Padraig Jouannisson|     Female| 43|792 James PlaceSu...|    7271|South Cameronborough|Australian Capita...|Australia|\n",
      "|        518|     Margalo Cousen|    Agender| 31|97 Isaac SummitSu...|    6393|            Bauchton|  Northern Territory|Australia|\n",
      "|        764|        Brod Phifer| Polygender| 32|05 Fahey CircleSu...|    9998|       Hettingerfort|   Western Australia|Australia|\n",
      "|        915|   Glenine Saladine|       Male| 70|9738 Liam RoadApt...|    9647|            Lukefort|Australian Capita...|Australia|\n",
      "|        969|    Ginnie Glasheen| Polygender| 77|08 Howe Boulevard...|    9561|         West Stella|     South Australia|Australia|\n",
      "|         36|   Xever Giacomelli|Genderqueer| 27|41 Durgan IslandA...|    5691|          Hamishfurt|          Queensland|Australia|\n",
      "|        595|   Thomasine Baudet|     Female| 60|313 Grace LoopApt...|     939|          Ameliaview|     South Australia|Australia|\n",
      "|        822|       Zena McKeown|    Agender| 59|41 Jenkins KnollS...|    2588|     Lake Andrewport|     South Australia|Australia|\n",
      "|        159|    Marga Longhurst|   Bigender| 73|497 Gibson Terrac...|     482|          Bellaburgh|     South Australia|Australia|\n",
      "|        196|    Erroll Michelle|Genderqueer| 25|694 Isabella WayS...|    4911|             New Ava|     South Australia|Australia|\n",
      "|        582| Amberly Seckington|     Female| 65|5260 Ward IslandA...|     429|            Skyeland|     South Australia|Australia|\n",
      "|        791|     Jojo Toffanini|    Agender| 34|3285 Kai MewsApt....|    8839|         Elijahshire|          Queensland|Australia|\n",
      "|        807|      Dena Banbrick| Polygender| 26|80 Adam HillSuite...|     953|          Tylershire|          Queensland|Australia|\n",
      "|        958|  Amelita Witterick|    Agender| 58|20 Michael GroveA...|    1321|         North Molly|  Northern Territory|Australia|\n",
      "|        325|   Robinetta Deners|Genderqueer| 68|32 Chase Junction...|    8304|        West Claudia|     South Australia|Australia|\n",
      "|        437|       Fifine Baron|Genderfluid| 23|70 Jerde GroveSui...|    8456|            Natefort|Australian Capita...|Australia|\n",
      "|        865|       Ondrea Apark|    Agender| 44|97 Rowe CrestApt....|    6913|         North Riley|            Victoria|Australia|\n",
      "|        988|       Tommie Benoy| Polygender| 35|1130 Turner Estat...|    9386|           New Harry|   Western Australia|Australia|\n",
      "|        155|        Myrle Kelby|       Male| 80|85 Charles MallSu...|    7841|         Port Sophia|     New South Wales|Australia|\n",
      "|        398|   Damiano Leyborne| Non-binary| 50|6166 Durgan WayAp...|    5497|        Gabrielville|   Western Australia|Australia|\n",
      "|        446|      Benny Songist| Non-binary| 47|165 Hermiston Loo...|    1552|     North Lukeshire|            Tasmania|Australia|\n",
      "|         63|       Kleon Trobey| Non-binary| 78|013 Edward Parade...|    5140|            Kossfort|            Victoria|Australia|\n",
      "|         77|      Angela Kester|       Male| 62|63 Tremblay Boule...|    1102|          Chloeville|     South Australia|Australia|\n",
      "|        192|    Tommy Ebunoluwa|     Female| 63|56 Wolf TerraceSu...|    4239|          East Mason|            Tasmania|Australia|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Handling Missing Values\n",
    "# Drop rows with any null values\n",
    "df_customers_cleaned = df_customers.dropna()\n",
    "\n",
    "# Handling Duplicates\n",
    "df_customers_cleaned =df_customers_cleaned.dropDuplicates()\n",
    "\n",
    "# Handling Outliers (Example: Removing outliers in 'age' column)\n",
    "# Assuming outliers are defined as age < 0 or age > 100\n",
    "df_customerscleaned =df_customers_cleaned.filter((col(\"age\") >= 0) & (col(\"age\") <= 100))\n",
    "df_customerscleaned.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c908dd-acf2-417d-afd4-fab29c1af14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|customer_id|      customer_name|     gender|age|        home_address|zip_code|                city|               state|  country|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "|        152|    Panchito Wybern|     Female| 79|03 Glover TrackAp...|    7845|       Christianport|Australian Capita...|Australia|\n",
      "|        540|       Maude McCaig|Genderfluid| 75|297 Ferry LaneApt...|    8550|         East Amelia|     South Australia|Australia|\n",
      "|        851|      Worthy Pardoe| Non-binary| 71|71 Joseph TrailAp...|    6648|     North Emmashire|Australian Capita...|Australia|\n",
      "|        903|   Leonhard Webland|       Male| 48|9694 Nathan Trail...|    2370|          Walkerfurt|     New South Wales|Australia|\n",
      "|        941|   Egon Figliovanni|     Female| 59|617 Morrison Cres...|     105|      Farrellchester|     New South Wales|Australia|\n",
      "|        227|    Isaak Ferrarone| Non-binary| 26|074 Hackett Parad...|    3857|         South Holly|            Tasmania|Australia|\n",
      "|        484|Padraig Jouannisson|     Female| 43|792 James PlaceSu...|    7271|South Cameronborough|Australian Capita...|Australia|\n",
      "|        518|     Margalo Cousen|    Agender| 31|97 Isaac SummitSu...|    6393|            Bauchton|  Northern Territory|Australia|\n",
      "|        764|        Brod Phifer| Polygender| 32|05 Fahey CircleSu...|    9998|       Hettingerfort|   Western Australia|Australia|\n",
      "|        915|   Glenine Saladine|       Male| 70|9738 Liam RoadApt...|    9647|            Lukefort|Australian Capita...|Australia|\n",
      "|        969|    Ginnie Glasheen| Polygender| 77|08 Howe Boulevard...|    9561|         West Stella|     South Australia|Australia|\n",
      "|         36|   Xever Giacomelli|Genderqueer| 27|41 Durgan IslandA...|    5691|          Hamishfurt|          Queensland|Australia|\n",
      "|        595|   Thomasine Baudet|     Female| 60|313 Grace LoopApt...|     939|          Ameliaview|     South Australia|Australia|\n",
      "|        822|       Zena McKeown|    Agender| 59|41 Jenkins KnollS...|    2588|     Lake Andrewport|     South Australia|Australia|\n",
      "|        159|    Marga Longhurst|   Bigender| 73|497 Gibson Terrac...|     482|          Bellaburgh|     South Australia|Australia|\n",
      "|        196|    Erroll Michelle|Genderqueer| 25|694 Isabella WayS...|    4911|             New Ava|     South Australia|Australia|\n",
      "|        582| Amberly Seckington|     Female| 65|5260 Ward IslandA...|     429|            Skyeland|     South Australia|Australia|\n",
      "|        791|     Jojo Toffanini|    Agender| 34|3285 Kai MewsApt....|    8839|         Elijahshire|          Queensland|Australia|\n",
      "|        807|      Dena Banbrick| Polygender| 26|80 Adam HillSuite...|     953|          Tylershire|          Queensland|Australia|\n",
      "|        958|  Amelita Witterick|    Agender| 58|20 Michael GroveA...|    1321|         North Molly|  Northern Territory|Australia|\n",
      "+-----------+-------------------+-----------+---+--------------------+--------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count rows before dropping duplicates\n",
    "original_count = df_customers.count()\n",
    "\n",
    "# Drop duplicates based on all columns\n",
    "df_no_duplicates = df_customers.dropDuplicates()\n",
    "\n",
    "# Count rows after dropping duplicates\n",
    "no_duplicates_count = df_no_duplicates.count()\n",
    "\n",
    "# Check if there were duplicates\n",
    "if original_count == no_duplicates_count:\n",
    "    print(\"No duplicates found.\")\n",
    "else:\n",
    "    print(f\"Duplicates found and removed. Original count: {original_count}, Count after removing duplicates: {no_duplicates_count}\")\n",
    "\n",
    "# Show the cleaned DataFrame without duplicates\n",
    "df_no_duplicates.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52df1e38-9f45-429f-b190-f14aeaef7b77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_customers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m countDistinct\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Group by 'Country' column and aggregate with countDistinct on 'customer_id'\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m country_counts \u001b[38;5;241m=\u001b[39m df_customers\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(countDistinct(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_count\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Show the results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m country_counts\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_customers' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "# Group by 'Country' column and aggregate with countDistinct on 'customer_id'\n",
    "country_counts = df_customers.groupBy('state').agg(countDistinct('customer_id').alias('state_count'))\n",
    "# Show the results\n",
    "country_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42cc40c-f61c-4414-970e-0edab9a570b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------------------+\n",
      "|               state|customer_count|           avg_age|\n",
      "+--------------------+--------------+------------------+\n",
      "|  Northern Territory|           125|            49.168|\n",
      "|     South Australia|           139| 49.86330935251799|\n",
      "|   Western Australia|           124| 48.70161290322581|\n",
      "|            Victoria|           121| 47.83471074380165|\n",
      "|     New South Wales|           132| 52.88636363636363|\n",
      "|Australian Capita...|           121|50.710743801652896|\n",
      "|            Tasmania|           104| 48.59615384615385|\n",
      "|          Queensland|           134|50.634328358208954|\n",
      "+--------------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, avg\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerCountryStats\").getOrCreate()\n",
    "\n",
    "# Assuming 'df_customers' is your DataFrame\n",
    "\n",
    "# Group by 'Country' column and aggregate with countDistinct on 'customer_id' and average on 'age'\n",
    "country_stats = df_customers.groupBy('state').agg(\n",
    "    countDistinct('customer_id').alias('customer_count'),\n",
    "    avg('age').alias('avg_age')\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "country_stats.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b20f5-05b6-44e4-a811-41484f58f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "\n",
    "\n",
    "# Calculate total amount spent by each customer\n",
    "df_total_amount_by_customer = df_sales.withColumn(\"total_amount\", col(\"price_per_unit\") * col(\"quantity\")) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(sum(\"total_amount\").alias(\"total_amount_spent\"))\n",
    "\n",
    "# Show the DataFrame with total amount spent by each customer\n",
    "df_total_amount_by_customer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4df3f3a4-c7fa-4fe5-95b3-7f5e665c9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|               state|customer_count|\n",
      "+--------------------+--------------+\n",
      "|     South Australia|           139|\n",
      "|          Queensland|           134|\n",
      "|     New South Wales|           132|\n",
      "|  Northern Territory|           125|\n",
      "|   Western Australia|           124|\n",
      "|            Victoria|           121|\n",
      "|Australian Capita...|           121|\n",
      "|            Tasmania|           104|\n",
      "+--------------------+--------------+\n",
      "\n",
      "State with the most customers: South Australia (139 customers)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "# Group by city and count the number of customers in each city\n",
    "city_customer_count = df_customers.groupBy(\"state\").agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "# Sort cities by customer count in descending order\n",
    "sorted_cities = city_customer_count.orderBy(F.desc(\"customer_count\"))\n",
    "sorted_cities.show()\n",
    "most_customers_city = sorted_cities.select(\"state\", \"customer_count\").first()\n",
    "# Show the city with the most customer count\n",
    "print(f\"State with the most customers: {most_customers_city['state']} ({most_customers_city['customer_count']} customers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bee2425-f9ac-4c5a-93a6-dc305c186449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---------+\n",
      "|               state|customer_count|total_age|\n",
      "+--------------------+--------------+---------+\n",
      "|  Northern Territory|           125|     6146|\n",
      "|     South Australia|           139|     6931|\n",
      "|   Western Australia|           124|     6039|\n",
      "|            Victoria|           121|     5788|\n",
      "|     New South Wales|           132|     6981|\n",
      "|Australian Capita...|           121|     6136|\n",
      "|            Tasmania|           104|     5054|\n",
      "|          Queensland|           134|     6785|\n",
      "+--------------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import countDistinct, sum\n",
    "# Group by 'Country' column and aggregate with countDistinct on 'customer_id' and sum on 'age'\n",
    "country_stats = df_customers.groupBy('state').agg(\n",
    "    countDistinct('customer_id').alias('customer_count'),\n",
    "    sum('age').alias('total_age')\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "country_stats.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a316f7cb-5439-4ddc-8ab9-f41b269bcfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|unique_states|\n",
      "+-------------+\n",
      "|            8|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|unique_cities|\n",
      "+-------------+\n",
      "|          961|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Count the number of unique states\n",
    "unique_states_df = df_customers.agg(F.countDistinct(\"state\").alias(\"unique_states\"))\n",
    "unique_states_df.show()\n",
    "# Count the number of unique cities\n",
    "unique_cities_df = df_customers.agg(F.countDistinct(\"city\").alias(\"unique_cities\"))\n",
    "unique_cities_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8b241e0-ac97-48e0-a019-aebaa6cdc711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            city|\n",
      "+----------------+\n",
      "|      Lake Lucas|\n",
      "|     Sanfordberg|\n",
      "|      Port Darcy|\n",
      "|  Hackettchester|\n",
      "| Lake Lillyville|\n",
      "|     Lake Joshua|\n",
      "|  South Zacmouth|\n",
      "|   Dickinsonside|\n",
      "| New Alexchester|\n",
      "|      Chloeville|\n",
      "|   Turnerborough|\n",
      "|      Walterfurt|\n",
      "|      Lake Jesse|\n",
      "|    Samanthaberg|\n",
      "|Lake Callumville|\n",
      "|      Justinport|\n",
      "|      Audreyport|\n",
      "|      Evansburgh|\n",
      "|        West Kai|\n",
      "|Greenfeldershire|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the distinct city names\n",
    "distinct_cities_df = df_customers.select(\"city\").distinct()\n",
    "\n",
    "# Show the distinct city names\n",
    "distinct_cities_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63a2d30f-fe77-4b30-8669-5ec7d28ab558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|average_age|\n",
      "+-----------+\n",
      "|      49.86|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate the average age of customers\n",
    "avg_age_df = df_customers.agg(F.avg(\"age\").alias(\"average_age\"))\n",
    "avg_age_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7c9f4c3-2745-46cf-991a-12743652bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               state|\n",
      "+--------------------+\n",
      "|  Northern Territory|\n",
      "|     South Australia|\n",
      "|   Western Australia|\n",
      "|            Victoria|\n",
      "|     New South Wales|\n",
      "|Australian Capita...|\n",
      "|            Tasmania|\n",
      "|          Queensland|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the distinct city names\n",
    "distinct_cities_df = df_customers.select(\"state\").distinct()\n",
    "\n",
    "# Show the distinct city names\n",
    "distinct_cities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb158be5-b8e5-40d3-97ab-2f671a92952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+----------+-------------+----------------------+-------------------+-----------+---+------------------------------+--------+------------------+------------------+---------+\n",
      "|customer_id|order_id|payment|order_date|delivery_date|total_amount_per_order|customer_name      |gender     |age|home_address                  |zip_code|city              |state             |country  |\n",
      "+-----------+--------+-------+----------+-------------+----------------------+-------------------+-----------+---+------------------------------+--------+------------------+------------------+---------+\n",
      "|64         |1       |30811  |2021-08-30|2021-09-24   |1487                  |Annabella Devote   |Genderfluid|75 |4927 Alice MeadowApt. 960     |7787    |Sanfordborough    |South Australia   |Australia|\n",
      "|473        |2       |50490  |2021-02-03|2021-02-13   |1130                  |Lori Briars        |Male       |61 |531 Schmitt BoulevardApt. 010 |1744    |Annaton           |South Australia   |Australia|\n",
      "|774        |3       |46763  |2021-10-08|2021-11-03   |508                   |Ellynn Korba       |Genderqueer|34 |2096 Wilson MewsApt. 714      |8590    |West Jonathanshire|Tasmania          |Australia|\n",
      "|433        |4       |39782  |2021-05-06|2021-05-19   |976                   |Candis Roswarne    |Non-binary |46 |5777 Mayer PassApt. 881       |9728    |West Michaelport  |Tasmania          |Australia|\n",
      "|441        |5       |14719  |2021-03-23|2021-03-24   |2043                  |Artemas Vasilischev|Bigender   |53 |33 Richards JunctionApt. 478  |7650    |South Rileyview   |Western Australia |Australia|\n",
      "|800        |6       |16197  |2021-09-09|2021-10-05   |732                   |Katusha Ceney      |Bigender   |49 |3570 Sarah AvenueApt. 092     |4505    |Daughertyport     |Queensland        |Australia|\n",
      "|626        |7       |37666  |2021-04-05|2021-04-11   |523                   |Tann Scothron      |Genderqueer|64 |72 Hunter Station StApt. 220  |1807    |Walterstad        |New South Wales   |Australia|\n",
      "|58         |8       |28484  |2021-04-12|2021-05-01   |299                   |Aurie Margett      |Agender    |39 |260 Lily PassSuite 549        |2783    |Amelieburgh       |New South Wales   |Australia|\n",
      "|852        |9       |12896  |2021-05-01|2021-05-11   |1315                  |Lucas Cromly       |Polygender |67 |079 Chase MallApt. 902        |8460    |O'connertown      |Tasmania          |Australia|\n",
      "|659        |10      |21922  |2021-10-15|2021-10-16   |874                   |Ira Dafforne       |Non-binary |57 |14 O'neill View RdApt. 908    |9653    |Sophiemouth       |Tasmania          |Australia|\n",
      "|785        |11      |36624  |2021-06-15|2021-06-30   |461                   |Jillana Blankau    |Genderfluid|54 |82 Beahan BoulevardSuite 281  |2683    |Port Adamburgh    |Northern Territory|Australia|\n",
      "|120        |12      |55507  |2021-06-30|2021-07-11   |714                   |Didi Garton        |Polygender |60 |13 Ivy BoulevardSuite 234     |3554    |Carterbury        |Queensland        |Australia|\n",
      "|204        |13      |57810  |2021-10-15|2021-10-31   |1583                  |Bird McGarvie      |Female     |60 |889 Hermann Station StApt. 026|5120    |East Jakeberg     |New South Wales   |Australia|\n",
      "|957        |14      |21270  |2021-03-29|2021-04-12   |1254                  |Felic Marrian      |Male       |75 |5215 Mitchell TrackSuite 778  |4220    |Samanthaberg      |South Australia   |Australia|\n",
      "|468        |15      |15488  |2021-09-22|2021-10-08   |1365                  |Maegan Ashton      |Genderqueer|67 |06 Ebert CrestSuite 385       |5607    |West Isla         |Tasmania          |Australia|\n",
      "|564        |16      |36479  |2021-07-07|2021-07-08   |1786                  |Shoshana Redsell   |Bigender   |34 |65 Schroeder WayApt. 552      |4054    |North Marcusland  |Northern Territory|Australia|\n",
      "|614        |17      |51822  |2021-02-09|2021-02-11   |670                   |Sherwynd Batsford  |Polygender |45 |627 Weber PassApt. 498        |7529    |Samuelside        |New South Wales   |Australia|\n",
      "|299        |18      |22902  |2021-01-01|2021-01-03   |952                   |Anny Midden        |Non-binary |32 |810 Jasper CircleSuite 777    |89      |Lake Amelia       |Victoria          |Australia|\n",
      "|513        |19      |56754  |2021-08-30|2021-09-10   |1244                  |Corri Womack       |Non-binary |39 |051 White WayApt. 392         |2358    |Port Ali          |Tasmania          |Australia|\n",
      "|789        |20      |45129  |2021-01-13|2021-01-20   |1557                  |Bone Perche        |Genderqueer|36 |638 Thomas ManorApt. 038      |1438    |Aliceland         |Western Australia |Australia|\n",
      "+-----------+--------+-------+----------+-------------+----------------------+-------------------+-----------+---+------------------------------+--------+------------------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the join operation using customer_id\n",
    "df_merged2 = df_merged.join(df_customers, on='customer_id', how='left')\n",
    "\n",
    "# Show the merged DataFrame\n",
    "df_merged2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa56c232-c1a0-4e3a-b0fc-25cdca1863d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------------+\n",
      "|customer_id|customer_name      |total_amount_per_order|\n",
      "+-----------+-------------------+----------------------+\n",
      "|64         |Annabella Devote   |1487                  |\n",
      "|473        |Lori Briars        |1130                  |\n",
      "|774        |Ellynn Korba       |508                   |\n",
      "|433        |Candis Roswarne    |976                   |\n",
      "|441        |Artemas Vasilischev|2043                  |\n",
      "|800        |Katusha Ceney      |732                   |\n",
      "|626        |Tann Scothron      |523                   |\n",
      "|58         |Aurie Margett      |299                   |\n",
      "|852        |Lucas Cromly       |1315                  |\n",
      "|659        |Ira Dafforne       |874                   |\n",
      "|785        |Jillana Blankau    |461                   |\n",
      "|120        |Didi Garton        |714                   |\n",
      "|204        |Bird McGarvie      |1583                  |\n",
      "|957        |Felic Marrian      |1254                  |\n",
      "|468        |Maegan Ashton      |1365                  |\n",
      "|564        |Shoshana Redsell   |1786                  |\n",
      "|614        |Sherwynd Batsford  |670                   |\n",
      "|299        |Anny Midden        |952                   |\n",
      "|513        |Corri Womack       |1244                  |\n",
      "|789        |Bone Perche        |1557                  |\n",
      "+-----------+-------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_merged is your DataFrame containing merged data\n",
    "\n",
    "# Select specific columns to show\n",
    "df_selected = df_merged2.select(\"customer_id\", \"customer_name\", \"total_amount_per_order\")\n",
    "\n",
    "# Show the selected columns\n",
    "df_selected.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acc66f85-df21-4eec-9dd8-cbd604b4309a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+-------------------+-------------+\n",
      "|order_id|customer_id|payment|order_date         |delivery_date|\n",
      "+--------+-----------+-------+-------------------+-------------+\n",
      "|1       |64         |30811  |2021-08-30 00:00:00|2021-09-24   |\n",
      "|2       |473        |50490  |2021-02-03 00:00:00|2021-02-13   |\n",
      "|3       |774        |46763  |2021-10-08 00:00:00|2021-11-03   |\n",
      "|4       |433        |39782  |2021-05-06 00:00:00|2021-05-19   |\n",
      "|5       |441        |14719  |2021-03-23 00:00:00|2021-03-24   |\n",
      "|6       |800        |16197  |2021-09-09 00:00:00|2021-10-05   |\n",
      "|7       |626        |37666  |2021-04-05 00:00:00|2021-04-11   |\n",
      "|8       |58         |28484  |2021-04-12 00:00:00|2021-05-01   |\n",
      "|9       |852        |12896  |2021-05-01 00:00:00|2021-05-11   |\n",
      "|10      |659        |21922  |2021-10-15 00:00:00|2021-10-16   |\n",
      "+--------+-----------+-------+-------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- delivery_date: date (nullable = true)\n",
      "\n",
      "Count of dataframe: 1000\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\orders.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "df_orders = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_orders.show(10,0)\n",
    "df_orders.printSchema()\n",
    "print(\"Count of dataframe:\",df_orders.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1143c0e2-a009-4d46-8ee1-3dc105f3d909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|customer_id|order_frequency|\n",
      "+-----------+---------------+\n",
      "|        471|              1|\n",
      "|        833|              2|\n",
      "|        463|              2|\n",
      "|        148|              1|\n",
      "|        496|              1|\n",
      "|        623|              3|\n",
      "|        737|              2|\n",
      "|        516|              1|\n",
      "|        251|              1|\n",
      "|         85|              1|\n",
      "|        808|              1|\n",
      "|        458|              1|\n",
      "|        883|              1|\n",
      "|        588|              2|\n",
      "|        799|              2|\n",
      "|        898|              2|\n",
      "|        970|              1|\n",
      "|        133|              1|\n",
      "|        853|              1|\n",
      "|        472|              2|\n",
      "|        513|              1|\n",
      "|         78|              2|\n",
      "|        918|              1|\n",
      "|        321|              1|\n",
      "|        673|              1|\n",
      "|        857|              1|\n",
      "|        974|              1|\n",
      "|        362|              1|\n",
      "|        876|              1|\n",
      "|        375|              2|\n",
      "+-----------+---------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "# Assuming df_orders is your DataFrame containing order data with 'customer_id' column\n",
    "# Group by 'customer_id' and aggregate with count on 'order_id' to get order frequency\n",
    "df_order_frequency = df_orders.groupBy('customer_id').agg(count('order_id').alias('order_frequency'))\n",
    "# Show the results\n",
    "df_order_frequency.show(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55de647b-91d9-43db-9f68-c065d5d1dda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+----------+-------------+---------------+\n",
      "|customer_id|order_id|payment|order_date|delivery_date|order_frequency|\n",
      "+-----------+--------+-------+----------+-------------+---------------+\n",
      "|         64|       1|  30811|2021-08-30|   2021-09-24|              1|\n",
      "|        473|       2|  50490|2021-02-03|   2021-02-13|              2|\n",
      "|        774|       3|  46763|2021-10-08|   2021-11-03|              3|\n",
      "|        433|       4|  39782|2021-05-06|   2021-05-19|              3|\n",
      "|        441|       5|  14719|2021-03-23|   2021-03-24|              2|\n",
      "+-----------+--------+-------+----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Assuming df_orders is your DataFrame containing order data with 'customer_id' column\n",
    "# Assuming df_order_frequency is your DataFrame containing order frequency data with 'customer_id' and 'order_frequency' columns\n",
    "\n",
    "# Perform the join based on 'customer_id'\n",
    "df_orders_with_frequency = df_orders.join(df_order_frequency, 'customer_id', 'left')\n",
    "\n",
    "# Show the joined DataFrame\n",
    "df_orders_with_frequency.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d9f975f-df7d-4a33-838f-bbc6ceb8ac65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- delivery_date: date (nullable = true)\n",
      "\n",
      "+--------+-----------+-------+----------+-------------+\n",
      "|order_id|customer_id|payment|order_date|delivery_date|\n",
      "+--------+-----------+-------+----------+-------------+\n",
      "|       1|         64|  30811|2021-08-30|   2021-09-24|\n",
      "|       2|        473|  50490|2021-02-03|   2021-02-13|\n",
      "|       3|        774|  46763|2021-10-08|   2021-11-03|\n",
      "|       4|        433|  39782|2021-05-06|   2021-05-19|\n",
      "|       5|        441|  14719|2021-03-23|   2021-03-24|\n",
      "|       6|        800|  16197|2021-09-09|   2021-10-05|\n",
      "|       7|        626|  37666|2021-04-05|   2021-04-11|\n",
      "|       8|         58|  28484|2021-04-12|   2021-05-01|\n",
      "|       9|        852|  12896|2021-05-01|   2021-05-11|\n",
      "|      10|        659|  21922|2021-10-15|   2021-10-16|\n",
      "|      11|        785|  36624|2021-06-15|   2021-06-30|\n",
      "|      12|        120|  55507|2021-06-30|   2021-07-11|\n",
      "|      13|        204|  57810|2021-10-15|   2021-10-31|\n",
      "|      14|        957|  21270|2021-03-29|   2021-04-12|\n",
      "|      15|        468|  15488|2021-09-22|   2021-10-08|\n",
      "|      16|        564|  36479|2021-07-07|   2021-07-08|\n",
      "|      17|        614|  51822|2021-02-09|   2021-02-11|\n",
      "|      18|        299|  22902|2021-01-01|   2021-01-03|\n",
      "|      19|        513|  56754|2021-08-30|   2021-09-10|\n",
      "|      20|        789|  45129|2021-01-13|   2021-01-20|\n",
      "+--------+-----------+-------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Assuming 'order_date' is your timestamp column\n",
    "df_orders= df_orders.withColumn('order_date', to_date(col('order_date')))\n",
    "\n",
    "# Show the updated schema to verify the data type change\n",
    "df_orders.printSchema()\n",
    "df_orders. show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10cba4b5-3c75-4a4a-911d-58b3a0c589cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+----------------------+------------+-------------------------+\n",
      "|unique_orders_count|unique_customers_count|unique_payment_methods|total_orders|orders_with_delivery_date|\n",
      "+-------------------+----------------------+----------------------+------------+-------------------------+\n",
      "|               1000|                   617|                   995|        1000|                     1000|\n",
      "+-------------------+----------------------+----------------------+------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregations for order_df DataFrame\n",
    "order_aggregations = df_orders.agg(\n",
    "    F.countDistinct(\"order_id\").alias(\"unique_orders_count\"),\n",
    "    F.countDistinct(\"customer_id\").alias(\"unique_customers_count\"),   \n",
    ")\n",
    "# Show the aggregated results\n",
    "order_aggregations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5c52bdd-6bd3-4a9c-ab6d-41e2d792702f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|order_date|count|\n",
      "+----------+-----+\n",
      "|2021-01-01|    3|\n",
      "|2021-01-02|    7|\n",
      "|2021-01-03|    3|\n",
      "|2021-01-04|    3|\n",
      "|2021-01-05|    5|\n",
      "|2021-01-06|    2|\n",
      "|2021-01-07|    5|\n",
      "|2021-01-08|    2|\n",
      "|2021-01-09|    5|\n",
      "|2021-01-10|    6|\n",
      "|2021-01-11|    4|\n",
      "|2021-01-12|    7|\n",
      "|2021-01-13|    4|\n",
      "|2021-01-14|    3|\n",
      "|2021-01-15|    3|\n",
      "|2021-01-16|    1|\n",
      "|2021-01-17|    2|\n",
      "|2021-01-18|    2|\n",
      "|2021-01-19|    1|\n",
      "|2021-01-20|    4|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_per_day = df_orders.groupBy(\"order_date\").count().orderBy(\"order_date\")\n",
    "orders_per_day.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de1c8174-3943-4952-91c8-3c55669d254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders.select('customer_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd7f0468-f6fe-425e-8c4c-06b7afbf5d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "|sales_id|order_id|product_id|price_per_unit|quantity|total_price|\n",
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "|0       |1       |218       |106           |2       |212        |\n",
      "|1       |1       |481       |118           |1       |118        |\n",
      "|2       |1       |2         |96            |3       |288        |\n",
      "|3       |1       |1002      |106           |2       |212        |\n",
      "|4       |1       |691       |113           |3       |339        |\n",
      "|5       |1       |981       |106           |3       |318        |\n",
      "|6       |2       |915       |96            |1       |96         |\n",
      "|7       |2       |686       |113           |1       |113        |\n",
      "|8       |2       |1091      |115           |3       |345        |\n",
      "|9       |2       |1196      |105           |1       |105        |\n",
      "+--------+--------+----------+--------------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price_per_unit: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- total_price: integer (nullable = true)\n",
      "\n",
      "Count of dataframe: 5000\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\sales.csv\"\n",
    "df_sales = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_sales.show(10,0)\n",
    "df_sales.printSchema()\n",
    "print(\"Count of dataframe:\",df_sales.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4de33642-a50d-4157-90be-7cfd30ad4ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------+\n",
      "|order_id|total_amount_per_order|\n",
      "+--------+----------------------+\n",
      "|1       |1487                  |\n",
      "|2       |1130                  |\n",
      "|3       |508                   |\n",
      "|4       |976                   |\n",
      "|5       |2043                  |\n",
      "|6       |732                   |\n",
      "|7       |523                   |\n",
      "|8       |299                   |\n",
      "|9       |1315                  |\n",
      "|10      |874                   |\n",
      "|11      |461                   |\n",
      "|12      |714                   |\n",
      "|13      |1583                  |\n",
      "|14      |1254                  |\n",
      "|15      |1365                  |\n",
      "|16      |1786                  |\n",
      "|17      |670                   |\n",
      "|18      |952                   |\n",
      "|19      |1244                  |\n",
      "|20      |1557                  |\n",
      "+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TotalAmountPerOrder\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming df_sales is your DataFrame with sales data\n",
    "# Calculate the total amount spent per order ID\n",
    "df_total_amount = df_sales.withColumn(\"total_amount\", col(\"price_per_unit\") * col(\"quantity\")) \\\n",
    "    .groupBy(\"order_id\") \\\n",
    "    .agg(sum(\"total_amount\").alias(\"total_amount_per_order\")) \\\n",
    "    .orderBy(\"order_id\")\n",
    "\n",
    "# Show the total amount spent per order ID\n",
    "df_total_amount.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "565b8218-7035-4846-b8c9-0fcbdbbb7562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+----------+-------------+----------------------+\n",
      "|order_id|customer_id|payment|order_date|delivery_date|total_amount_per_order|\n",
      "+--------+-----------+-------+----------+-------------+----------------------+\n",
      "|1       |64         |30811  |2021-08-30|2021-09-24   |1487                  |\n",
      "|2       |473        |50490  |2021-02-03|2021-02-13   |1130                  |\n",
      "|3       |774        |46763  |2021-10-08|2021-11-03   |508                   |\n",
      "|4       |433        |39782  |2021-05-06|2021-05-19   |976                   |\n",
      "|5       |441        |14719  |2021-03-23|2021-03-24   |2043                  |\n",
      "|6       |800        |16197  |2021-09-09|2021-10-05   |732                   |\n",
      "|7       |626        |37666  |2021-04-05|2021-04-11   |523                   |\n",
      "|8       |58         |28484  |2021-04-12|2021-05-01   |299                   |\n",
      "|9       |852        |12896  |2021-05-01|2021-05-11   |1315                  |\n",
      "|10      |659        |21922  |2021-10-15|2021-10-16   |874                   |\n",
      "|11      |785        |36624  |2021-06-15|2021-06-30   |461                   |\n",
      "|12      |120        |55507  |2021-06-30|2021-07-11   |714                   |\n",
      "|13      |204        |57810  |2021-10-15|2021-10-31   |1583                  |\n",
      "|14      |957        |21270  |2021-03-29|2021-04-12   |1254                  |\n",
      "|15      |468        |15488  |2021-09-22|2021-10-08   |1365                  |\n",
      "|16      |564        |36479  |2021-07-07|2021-07-08   |1786                  |\n",
      "|17      |614        |51822  |2021-02-09|2021-02-11   |670                   |\n",
      "|18      |299        |22902  |2021-01-01|2021-01-03   |952                   |\n",
      "|19      |513        |56754  |2021-08-30|2021-09-10   |1244                  |\n",
      "|20      |789        |45129  |2021-01-13|2021-01-20   |1557                  |\n",
      "+--------+-----------+-------+----------+-------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_total_amount is your DataFrame with total amount per order ID\n",
    "# Assuming df_orders is your DataFrame with order details\n",
    "\n",
    "# Perform the join operation\n",
    "df_merged = df_orders.join(df_total_amount, on='order_id', how='left')\n",
    "\n",
    "# Show the merged DataFrame\n",
    "df_merged.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72ce46ec-8017-479d-8ffb-e7312edc027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales .select('order_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e0e19a9-a479-49e9-84cf-109a0dbfdcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+\n",
      "|summary|   price_per_unit|          quantity|      total_price|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "|  count|             5000|              5000|             5000|\n",
      "|   mean|         103.5016|            1.9924|           206.36|\n",
      "| stddev|9.195004462283432|0.8075101575403906|86.35745666741475|\n",
      "|    min|               90|                 1|               90|\n",
      "|    max|              119|                 3|              357|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.describe([\"price_per_unit\", \"quantity\", \"total_price\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d03a99ab-02e3-47a4-b481-a26b9597a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|product_id|sum(total_price)|\n",
      "+----------+----------------+\n",
      "|        78|            2832|\n",
      "|       472|            2714|\n",
      "|       707|            2499|\n",
      "|       579|            2400|\n",
      "|       843|            2373|\n",
      "|       486|            2360|\n",
      "|       740|            2289|\n",
      "|       727|            2261|\n",
      "|       182|            2254|\n",
      "|       465|            2124|\n",
      "|        95|            2124|\n",
      "|        74|            2124|\n",
      "|       810|            2106|\n",
      "|       316|            2071|\n",
      "|       405|            2023|\n",
      "|       222|            2014|\n",
      "|       830|            1989|\n",
      "|      1188|            1980|\n",
      "|      1184|            1980|\n",
      "|      1091|            1955|\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_by_product = df_sales.groupBy('product_id').sum('total_price').orderBy('sum(total_price)', ascending=False)\n",
    "sales_by_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cc3052e-e3eb-48b1-a570-185322ed237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+---------------------+------------------+-------------+-------------------+\n",
      "|unique_sales_count|unique_orders_count|unique_products_count|avg_price_per_unit|total_revenue|total_quantity_sold|\n",
      "+------------------+-------------------+---------------------+------------------+-------------+-------------------+\n",
      "|              5000|                993|                 1233|          103.5016|   5155414696|               9962|\n",
      "+------------------+-------------------+---------------------+------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregations for df_sales DataFrame\n",
    "sales_aggregations = df_sales.agg(\n",
    "    F.countDistinct(\"sales_id\").alias(\"unique_sales_count\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"unique_orders_count\"),\n",
    "    F.countDistinct(\"product_id\").alias(\"unique_products_count\"),\n",
    "    F.avg(\"price_per_unit\").alias(\"avg_price_per_unit\"),\n",
    "    (F.sum(\"price_per_unit\") * F.sum(\"quantity\")).alias(\"total_revenue\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity_sold\")\n",
    ")\n",
    "\n",
    "# Show the aggregated results\n",
    "sales_aggregations.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc793219-97b5-43a1-815d-0980995b68b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2502828-2e86-4fc3-9504-23240492dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+----+------+-----+--------+-----------------------------------------------+\n",
      "|product_ID|product_type|product_name|size|colour|price|quantity|description                                    |\n",
      "+----------+------------+------------+----+------+-----+--------+-----------------------------------------------+\n",
      "|0         |Shirt       |Oxford Cloth|XS  |red   |114  |66      |A red coloured, XS sized, Oxford Cloth Shirt   |\n",
      "|1         |Shirt       |Oxford Cloth|S   |red   |114  |53      |A red coloured, S sized, Oxford Cloth Shirt    |\n",
      "|2         |Shirt       |Oxford Cloth|M   |red   |114  |54      |A red coloured, M sized, Oxford Cloth Shirt    |\n",
      "|3         |Shirt       |Oxford Cloth|L   |red   |114  |69      |A red coloured, L sized, Oxford Cloth Shirt    |\n",
      "|4         |Shirt       |Oxford Cloth|XL  |red   |114  |47      |A red coloured, XL sized, Oxford Cloth Shirt   |\n",
      "|5         |Shirt       |Oxford Cloth|XS  |orange|114  |45      |A orange coloured, XS sized, Oxford Cloth Shirt|\n",
      "|6         |Shirt       |Oxford Cloth|S   |orange|114  |72      |A orange coloured, S sized, Oxford Cloth Shirt |\n",
      "|7         |Shirt       |Oxford Cloth|M   |orange|114  |77      |A orange coloured, M sized, Oxford Cloth Shirt |\n",
      "|8         |Shirt       |Oxford Cloth|L   |orange|114  |48      |A orange coloured, L sized, Oxford Cloth Shirt |\n",
      "|9         |Shirt       |Oxford Cloth|XL  |orange|114  |43      |A orange coloured, XL sized, Oxford Cloth Shirt|\n",
      "+----------+------------+------------+----+------+-----+--------+-----------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- product_ID: integer (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- colour: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "Count of dataframe: 1260\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\OneDrive\\Desktop\\Sub\\BigData\\Csv. files\\products.csv\"\n",
    "df_products = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "# Show the first 10 rows of the DataFrame\n",
    "df_products.show(10,0)\n",
    "df_products.printSchema()\n",
    "print(\"Count of dataframe:\",df_products.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab56b5-c923-4637-9fae-268575e40484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47e2575a-1f70-4299-9320-7562983e0a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+----+------+-----+--------+-----------------------------------------------+-----------+\n",
      "|product_ID|product_type|product_name|size|colour|price|quantity|description                                    |total_price|\n",
      "+----------+------------+------------+----+------+-----+--------+-----------------------------------------------+-----------+\n",
      "|0         |Shirt       |Oxford Cloth|XS  |red   |114  |66      |A red coloured, XS sized, Oxford Cloth Shirt   |7524       |\n",
      "|1         |Shirt       |Oxford Cloth|S   |red   |114  |53      |A red coloured, S sized, Oxford Cloth Shirt    |6042       |\n",
      "|2         |Shirt       |Oxford Cloth|M   |red   |114  |54      |A red coloured, M sized, Oxford Cloth Shirt    |6156       |\n",
      "|3         |Shirt       |Oxford Cloth|L   |red   |114  |69      |A red coloured, L sized, Oxford Cloth Shirt    |7866       |\n",
      "|4         |Shirt       |Oxford Cloth|XL  |red   |114  |47      |A red coloured, XL sized, Oxford Cloth Shirt   |5358       |\n",
      "|5         |Shirt       |Oxford Cloth|XS  |orange|114  |45      |A orange coloured, XS sized, Oxford Cloth Shirt|5130       |\n",
      "|6         |Shirt       |Oxford Cloth|S   |orange|114  |72      |A orange coloured, S sized, Oxford Cloth Shirt |8208       |\n",
      "|7         |Shirt       |Oxford Cloth|M   |orange|114  |77      |A orange coloured, M sized, Oxford Cloth Shirt |8778       |\n",
      "|8         |Shirt       |Oxford Cloth|L   |orange|114  |48      |A orange coloured, L sized, Oxford Cloth Shirt |5472       |\n",
      "|9         |Shirt       |Oxford Cloth|XL  |orange|114  |43      |A orange coloured, XL sized, Oxford Cloth Shirt|4902       |\n",
      "|10        |Shirt       |Oxford Cloth|XS  |yellow|114  |72      |A yellow coloured, XS sized, Oxford Cloth Shirt|8208       |\n",
      "|11        |Shirt       |Oxford Cloth|S   |yellow|114  |78      |A yellow coloured, S sized, Oxford Cloth Shirt |8892       |\n",
      "|12        |Shirt       |Oxford Cloth|M   |yellow|114  |56      |A yellow coloured, M sized, Oxford Cloth Shirt |6384       |\n",
      "|13        |Shirt       |Oxford Cloth|L   |yellow|114  |75      |A yellow coloured, L sized, Oxford Cloth Shirt |8550       |\n",
      "|14        |Shirt       |Oxford Cloth|XL  |yellow|114  |50      |A yellow coloured, XL sized, Oxford Cloth Shirt|5700       |\n",
      "|15        |Shirt       |Oxford Cloth|XS  |green |114  |68      |A green coloured, XS sized, Oxford Cloth Shirt |7752       |\n",
      "|16        |Shirt       |Oxford Cloth|S   |green |114  |56      |A green coloured, S sized, Oxford Cloth Shirt  |6384       |\n",
      "|17        |Shirt       |Oxford Cloth|M   |green |114  |60      |A green coloured, M sized, Oxford Cloth Shirt  |6840       |\n",
      "|18        |Shirt       |Oxford Cloth|L   |green |114  |52      |A green coloured, L sized, Oxford Cloth Shirt  |5928       |\n",
      "|19        |Shirt       |Oxford Cloth|XL  |green |114  |42      |A green coloured, XL sized, Oxford Cloth Shirt |4788       |\n",
      "+----------+------------+------------+----+------+-----+--------+-----------------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calculate total price for each product\n",
    "df_total_price = df_products.withColumn(\"total_price\", col(\"quantity\") * col(\"price\"))\n",
    "\n",
    "# Show the DataFrame with total price\n",
    "df_total_price.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa641632-e2e6-4f64-9f18-cfee11cab8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----------+\n",
      "|product_type|colour|total_price|\n",
      "+------------+------+-----------+\n",
      "|Shirt       |red   |389840     |\n",
      "|Trousers    |orange|375139     |\n",
      "|Jacket      |orange|403578     |\n",
      "|Trousers    |violet|368337     |\n",
      "|Jacket      |green |389454     |\n",
      "|Trousers    |blue  |353392     |\n",
      "|Jacket      |red   |386938     |\n",
      "|Jacket      |violet|403096     |\n",
      "|Trousers    |yellow|367561     |\n",
      "|Shirt       |yellow|399579     |\n",
      "|Jacket      |yellow|377513     |\n",
      "|Shirt       |blue  |393531     |\n",
      "|Shirt       |indigo|392906     |\n",
      "|Jacket      |blue  |387448     |\n",
      "|Jacket      |indigo|381438     |\n",
      "|Shirt       |orange|382718     |\n",
      "|Trousers    |indigo|348804     |\n",
      "|Trousers    |red   |381457     |\n",
      "|Trousers    |green |364529     |\n",
      "|Shirt       |violet|393587     |\n",
      "+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total_price_by_type_color = df_products.withColumn(\"total_price\", col(\"quantity\") * col(\"price\")) \\\n",
    "    .groupBy(\"product_type\", \"colour\") \\\n",
    "    .agg(sum(\"total_price\").alias(\"total_price\"))\n",
    "\n",
    "# Show the DataFrame with total price by product type and color\n",
    "df_total_price_by_type_color.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11cdb38c-1499-4139-8940-ca0c7d2fcfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "|summary|      customer_id| customer_name|    gender|               age|        home_address|         zip_code|     city|               state|  country|\n",
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "|  count|             1000|          1000|      1000|              1000|                1000|             1000|     1000|                1000|     1000|\n",
      "|   mean|            500.5|          null|      null|             49.86|                null|         5004.872|     null|                null|     null|\n",
      "| stddev|288.8194360957494|          null|      null|17.647828360618387|                null|2884.497332027621|     null|                null|     null|\n",
      "|    min|                1|Abbot Rickaert|   Agender|                20|00 Fadel CircuitA...|                2|Aaronbury|Australian Capita...|Australia|\n",
      "|    max|             1000|   Zulema Teml|Polygender|                80|9993 Wood RidgeAp...|             9998| Zacville|   Western Australia|Australia|\n",
      "+-------+-----------------+--------------+----------+------------------+--------------------+-----------------+---------+--------------------+---------+\n",
      "\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|          sales_id|         order_id|       product_id|   price_per_unit|          quantity|      total_price|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|              5000|             5000|             5000|             5000|              5000|             5000|\n",
      "|   mean|            2499.5|         503.0382|         634.0532|         103.5016|            1.9924|           206.36|\n",
      "| stddev|1443.5200033252052|285.9644178397271|363.2557937928936|9.195004462283432|0.8075101575403906|86.35745666741475|\n",
      "|    min|                 0|                1|                1|               90|                 1|               90|\n",
      "|    max|              4999|              999|             1259|              119|                 3|              357|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.describe().show()\n",
    "df_sales.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5db392d2-6057-4d9e-aff0-2ca0a5c35b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------------------+--------------+\n",
      "|unique_products_count|total_products|     average_price|total_quantity|\n",
      "+---------------------+--------------+------------------+--------------+\n",
      "|                 1260|          1260|105.80555555555556|         75789|\n",
      "+---------------------+--------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregations for product DataFrame\n",
    "product_aggregations = df_products.agg(\n",
    "    F.countDistinct(\"product_ID\").alias(\"unique_products_count\"),\n",
    "    F.count(\"product_type\").alias(\"total_products\"),\n",
    "    F.avg(\"price\").alias(\"average_price\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity\")\n",
    ")\n",
    "\n",
    "# Show the aggregated results\n",
    "product_aggregations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "824e2ed8-6dc8-49c0-b04b-25bd37ddb06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|product_type|count|\n",
      "+------------+-----+\n",
      "|      Jacket|  420|\n",
      "|    Trousers|  420|\n",
      "|       Shirt|  420|\n",
      "+------------+-----+\n",
      "\n",
      "+------------+------------------+\n",
      "|product_type|         avg_price|\n",
      "+------------+------------------+\n",
      "|      Jacket|107.41666666666667|\n",
      "|    Trousers|101.66666666666667|\n",
      "|       Shirt|108.33333333333333|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count of Products by Type\n",
    "product_type_counts = df_products.groupBy(\"product_type\").count().orderBy(\"count\", ascending=False)\n",
    "# Show product type counts\n",
    "product_type_counts.show()\n",
    "# Statistical Analysis by Product Type (example: average price)\n",
    "avg_price_by_type = df_products.groupBy(\"product_type\").agg(F.avg(\"price\").alias(\"avg_price\"))\n",
    "# Show average price by product type\n",
    "avg_price_by_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592424d0-27f8-4fc9-80db-73b9f0cfd27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67e4045c-19b8-4be5-9299-daa5cce696bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+----------+-------------+-----------------+--------+---+--------------------+--------+-------------+------------------+---------+\n",
      "|customer_id|order_id|payment|order_date|delivery_date|    customer_name|  gender|age|        home_address|zip_code|         city|             state|  country|\n",
      "+-----------+--------+-------+----------+-------------+-----------------+--------+---+--------------------+--------+-------------+------------------+---------+\n",
      "|          1|     729|  35593|2021-02-18|   2021-03-01|    Leanna Busson|  Female| 30|8606 Victoria Ter...|    5464|Johnstonhaven|Northern Territory|Australia|\n",
      "|          1|     670|  10246|2021-03-06|   2021-04-01|    Leanna Busson|  Female| 30|8606 Victoria Ter...|    5464|Johnstonhaven|Northern Territory|Australia|\n",
      "|          1|     455|  24550|2021-04-04|   2021-04-06|    Leanna Busson|  Female| 30|8606 Victoria Ter...|    5464|Johnstonhaven|Northern Territory|Australia|\n",
      "|          7|     465|  48935|2021-05-21|   2021-05-23|  Winslow Ewbanck|Bigender| 76|92 Hills Station ...|     793|    Masonfurt|        Queensland|Australia|\n",
      "|         10|     595|  45626|2021-03-09|   2021-03-29|Susanetta Wilshin|Bigender| 70|615 Hayley KnollS...|    2118|    Joelburgh| Western Australia|Australia|\n",
      "+-----------+--------+-------+----------+-------------+-----------------+--------+---+--------------------+--------+-------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming df_orders and df_customers are your Spark DataFrames\n",
    "\n",
    "# Perform inner join on 'customer_id' column\n",
    "merged_df = df_orders.join(df_customers, on='customer_id', how='inner')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "merged_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4cd34-f797-432d-a6c9-8e1a1f3ce388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
